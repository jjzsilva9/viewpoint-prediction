{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "caac3c60-cbd9-4794-a8c0-ced5b193b331",
   "metadata": {},
   "source": [
    "<h1>CNN Model</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c437917-e990-4491-a25e-b7adeb77f372",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycocotools.coco import COCO\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as functions\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import re\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9003254-3d2c-4f84-91a9-0c6d966f104e",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATADIR = \"cocodoom/\"\n",
    "USED_RUNS = [\"run1\", \"run2\", \"run3\"]\n",
    "\n",
    "dataSplit, TRAIN_RUN = \"run-full-train\", \"run1\"\n",
    "\n",
    "annFile = '{}{}.json'.format(DATADIR,dataSplit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8df8e50-550c-43a3-8b8e-b5a5e41ed0b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=20.40s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "coco_train = COCO(annFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f281e0a-c5f6-4c4c-9d31-00090dce5487",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataSplit, VAL_RUN = \"run-full-val\", \"run2\"\n",
    "\n",
    "annFile = '{}{}.json'.format(DATADIR,dataSplit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78145555-2459-49ef-a223-2e0033d71076",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=27.77s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "coco_val = COCO(annFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d54d8da-8f03-492e-aa8e-afe2122d3e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataSplit, TEST_RUN = \"run-full-test\", \"run3\"\n",
    "\n",
    "annFile = '{}{}.json'.format(DATADIR,dataSplit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a281c69-85fb-4aa1-adc1-bd4027b8f8c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=16.26s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "coco_test = COCO(annFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b2fdf2f3-61ca-4528-909c-5101c530783f",
   "metadata": {},
   "outputs": [],
   "source": [
    "player_positions = {\"run1\":{}, \"run2\":{}, \"run3\":{}}\n",
    "\n",
    "for run in USED_RUNS:\n",
    "    with open(DATADIR+run+\"/log.txt\", 'r') as log_file:\n",
    "        for line in log_file:\n",
    "            if \"player\" in line:\n",
    "                line = line.strip()\n",
    "                tic, stats = line.split(\"player:\")\n",
    "                x, y, z, angle = stats.split(\",\")\n",
    "    \n",
    "                # Store position in the dictionary\n",
    "                player_positions[run][int(tic)] = (float(x), float(y), float(z), float(angle))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a78dd7f8-f3a5-4046-a31d-46f5c3cc747e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoomMotionDataset(Dataset):\n",
    "    def __init__(self, coco, run, transform=None):\n",
    "        self.coco = coco\n",
    "        self.run = run\n",
    "        self.img_ids = coco.getImgIds()\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_ids)\n",
    "\n",
    "    def getSegmentationMask(self, rgb_filename):\n",
    "        return rgb_filename.replace(\"rgb\", \"objects\")\n",
    "\n",
    "    def getDepthMask(self, rgb_filename):\n",
    "        return rgb_filename.replace(\"rgb\", \"depth\")\n",
    "\n",
    "    def color_to_index(self, segmentation_image):\n",
    "        # Map colors to class indices\n",
    "        r, g, b = segmentation_image\n",
    "        pixel_values = r + (g *  2**8) + (b * 2**16)  # From cocodoom documentation, converts to an object id\n",
    "\n",
    "        class_map = torch.full_like(pixel_values, 3, dtype=torch.long)\n",
    "\n",
    "        sky = (1 << 23) + 0\n",
    "        horizontal = (1 << 23) + 1\n",
    "        vertical = (1 << 23) + 2\n",
    "        \n",
    "        class_map[x == sky] = 0\n",
    "        class_map[x == horizontal] = 1\n",
    "        class_map[x == vertical] = 2\n",
    "        return class_map\n",
    "\n",
    "    def load_image(self, path):\n",
    "        img = Image.open(path)\n",
    "        return transforms.ToTensor()(img)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load the RGB image\n",
    "        rgb_filename = self.coco.loadImgs(self.img_ids[idx])[0]['file_name']\n",
    "        tic = int(rgb_filename.replace(\".png\", \"\").split(\"/\")[-1])\n",
    "        next_tic = tic+1\n",
    "        previous_tic = tic-1\n",
    "\n",
    "        player_position = player_positions[self.run][tic]\n",
    "        if next_tic not in player_positions[self.run]:\n",
    "            next_player_position = player_position\n",
    "        else:\n",
    "            next_player_position = player_positions[self.run][next_tic]\n",
    "        dx = next_player_position[0] - player_position[0]\n",
    "        dy = next_player_position[1] - player_position[1]\n",
    "        dz = next_player_position[2] - player_position[2]\n",
    "        dangle = np.pi - abs(abs(next_player_position[3] - player_position[3]) - np.pi)\n",
    "        \n",
    "        dx_relative = dx * np.cos(2 * np.pi - player_position[3]) + dy * np.cos(player_position[3] - 1/2 * np.pi)\n",
    "        dy_relative = dx * np.sin(2 * np.pi - player_position[3]) + dy * np.sin(player_position[3] - 1/2 * np.pi)\n",
    "        next_motion_vector = (dx_relative, dy_relative, dz, dangle)\n",
    "\n",
    "        if previous_tic not in player_positions[self.run]:\n",
    "            prev_player_position = player_position\n",
    "        else :\n",
    "            prev_player_position = player_positions[self.run][previous_tic]\n",
    "        dx = player_position[0] - prev_player_position[0]\n",
    "        dy = player_position[1] - prev_player_position[1]\n",
    "        dz = player_position[2] - prev_player_position[2]\n",
    "        dangle = np.pi - abs(abs(player_position[3] - prev_player_position[3]) - np.pi)\n",
    "        \n",
    "        dx_relative = dx * np.cos(2 * np.pi - prev_player_position[3]) + dy * np.cos(prev_player_position[3] - 1/2 * np.pi)\n",
    "        dy_relative = dx * np.sin(2 * np.pi - prev_player_position[3]) + dy * np.sin(prev_player_position[3] - 1/2 * np.pi)\n",
    "        prev_motion_vector = (dx_relative, dy_relative, dz, dangle)\n",
    "            \n",
    "        \n",
    "        # Load and process the segmentation map\n",
    "        seg_image = self.load_image(self.getSegmentationMask(DATADIR + rgb_filename))\n",
    "        seg_class_map = self.color_to_index(seg_image)\n",
    "        seg_class_one_hot = functions.one_hot(seg_class_map, num_classes=4).to(dtype=torch.float).permute(2, 0, 1)\n",
    "        #print(seg_class_one_hot.shape)\n",
    "        #seg_class_mode = self.mode_pooling(seg_class_one_hot, 2)\n",
    "        #print(seg_class_mode.shape)\n",
    "\n",
    "        # Load depth map\n",
    "        depth_mask = self.load_image(self.getDepthMask(DATADIR + rgb_filename))\n",
    "        #depth_mask_mode = self.mode_pooling(depth_mask, 2)\n",
    "        #print(depth_mask.shape)\n",
    "\n",
    "        # Combine depth and segmentation as separate channels\n",
    "        combined = torch.cat([seg_class_one_hot, depth_mask], dim=0)\n",
    "        \n",
    "        # Motion vector (label)\n",
    "        next_motion_vector = torch.tensor(next_motion_vector, dtype=torch.float32)\n",
    "        prev_motion_vector = torch.tensor(prev_motion_vector, dtype=torch.float32)\n",
    "        \n",
    "        return {\"image\": combined, \"past_motion\": prev_motion_vector, \"target\": next_motion_vector}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8680de85-df84-4334-87be-8defcfc6e6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "  def __init__(self, activation_function=functions.relu, device=torch.device(\"cpu\")):\n",
    "    super(NeuralNetwork, self).__init__()\n",
    "\n",
    "    # Max pooling like this should preserve all of the present classes in a 2x2 window into one vector.\n",
    "    self.pool = nn.MaxPool2d(kernel_size=(2, 2)).to(device)\n",
    "      \n",
    "    self.efficient_net = torchvision.models.efficientnet_b0().to(device)\n",
    "\n",
    "    self.efficient_net.features[0][0] = nn.Conv2d(5, 32, kernel_size=3, stride=2, padding=1, bias=False).to(device)\n",
    "    num_features = self.efficient_net.classifier[1].in_features\n",
    "    self.efficient_net.classifier[1] = nn.Identity()\n",
    "\n",
    "    self.motion_embedding = nn.Linear(4, num_features).to(device)\n",
    "\n",
    "    self.classification = nn.Linear(num_features*2, 4).to(device)\n",
    "\n",
    "  def forward(self, image_data, prev_motion):\n",
    "    x = self.pool(image_data)\n",
    "    x = self.efficient_net(x)\n",
    "    y = self.motion_embedding(prev_motion)\n",
    "    x = torch.cat((x, y), dim=1)\n",
    "    x = self.classification(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "81129192-3f18-4f79-b559-be1db7b544d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "poch 1/20: 100%|█████████████| 3964/3964 [1:32:24<00:00,  1.40s/batch, batch_loss=49, batch_index=3964, batch_size=28]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 2110.1263\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "poch 2/20: 100%|███████████| 3964/3964 [1:37:24<00:00,  1.47s/batch, batch_loss=44.8, batch_index=3964, batch_size=28]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Loss: 766.9389\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "poch 3/20: 100%|███████████| 3964/3964 [1:35:47<00:00,  1.45s/batch, batch_loss=44.4, batch_index=3964, batch_size=28]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Loss: 632.9967\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "poch 4/20: 100%|███████████| 3964/3964 [1:35:39<00:00,  1.45s/batch, batch_loss=55.2, batch_index=3964, batch_size=28]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Loss: 544.3366\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "poch 5/20: 100%|███████████| 3964/3964 [1:35:11<00:00,  1.44s/batch, batch_loss=54.6, batch_index=3964, batch_size=28]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Loss: 515.3879\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "poch 6/20: 100%|███████████| 3964/3964 [1:34:22<00:00,  1.43s/batch, batch_loss=49.3, batch_index=3964, batch_size=28]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Loss: 508.9930\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "poch 7/20: 100%|███████████| 3964/3964 [1:34:10<00:00,  1.43s/batch, batch_loss=56.1, batch_index=3964, batch_size=28]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Loss: 508.6478\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "poch 8/20: 100%|███████████| 3964/3964 [1:34:30<00:00,  1.43s/batch, batch_loss=56.5, batch_index=3964, batch_size=28]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Loss: 506.2129\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "poch 9/20: 100%|███████████| 3964/3964 [1:31:15<00:00,  1.38s/batch, batch_loss=54.2, batch_index=3964, batch_size=28]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Loss: 510.8113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "poch 10/20: 100%|██████████| 3964/3964 [1:28:23<00:00,  1.34s/batch, batch_loss=56.8, batch_index=3964, batch_size=28]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 506.9681\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "poch 11/20: 100%|██████████| 3964/3964 [1:27:41<00:00,  1.33s/batch, batch_loss=55.8, batch_index=3964, batch_size=28]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11, Loss: 507.7137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "poch 12/20: 100%|██████████| 3964/3964 [1:29:56<00:00,  1.36s/batch, batch_loss=61.2, batch_index=3964, batch_size=28]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12, Loss: 510.1210\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "poch 13/20: 100%|████████████| 3964/3964 [1:28:34<00:00,  1.34s/batch, batch_loss=51, batch_index=3964, batch_size=28]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13, Loss: 508.5548\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "poch 14/20: 100%|██████████| 3964/3964 [1:28:19<00:00,  1.34s/batch, batch_loss=47.7, batch_index=3964, batch_size=28]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14, Loss: 508.4461\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "poch 15/20: 100%|██████████| 3964/3964 [1:29:26<00:00,  1.35s/batch, batch_loss=50.5, batch_index=3964, batch_size=28]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15, Loss: 508.9400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "poch 16/20: 100%|██████████| 3964/3964 [1:30:08<00:00,  1.36s/batch, batch_loss=46.6, batch_index=3964, batch_size=28]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16, Loss: 507.3291\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "poch 17/20: 100%|██████████| 3964/3964 [1:29:57<00:00,  1.36s/batch, batch_loss=40.7, batch_index=3964, batch_size=28]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17, Loss: 506.9990\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "poch 18/20: 100%|██████████| 3964/3964 [1:29:19<00:00,  1.35s/batch, batch_loss=46.1, batch_index=3964, batch_size=28]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18, Loss: 508.8963\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "poch 19/20: 100%|██████████| 3964/3964 [1:30:59<00:00,  1.38s/batch, batch_loss=36.8, batch_index=3964, batch_size=28]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19, Loss: 507.8594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/20: 100%|██████████| 3964/3964 [1:31:41<00:00,  1.39s/batch, batch_loss=38.7, batch_index=3964, batch_size=28]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20, Loss: 509.2219\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "batch_size = 64\n",
    "learning_rate = 1e-3\n",
    "num_epochs = 20\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = NeuralNetwork(device=device).to(device)\n",
    "\n",
    "train_dataset = DoomMotionDataset(coco_train, TRAIN_RUN)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "val_dataset = DoomMotionDataset(coco_val, VAL_RUN)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\", unit=\"batch\")\n",
    "    for batch_idx, batch in enumerate(progress_bar):\n",
    "        image, past_motion, targets = batch[\"image\"], batch[\"past_motion\"], batch[\"target\"]\n",
    "        image, past_motion, targets = image.to(device), past_motion.to(device), targets.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(image, past_motion)\n",
    "    \n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        progress_bar.set_postfix({\n",
    "            \"batch_loss\": loss.item(),\n",
    "            \"batch_index\": batch_idx + 1,\n",
    "            \"batch_size\": image.size(0)\n",
    "        })\n",
    "\n",
    "    # Average loss per epoch\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1}, Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "    model_weight_path = f\"image_motion_cnn_{epoch}epoch.pth\"\n",
    "    torch.save(model.state_dict(), model_weight_path)\n",
    "\n",
    "    # model.eval()  # Set the model to evaluation mode\n",
    "    # running_loss = 0.0\n",
    "    \n",
    "    \n",
    "    # progress_bar = tqdm(val_loader, desc=\"Validation\", unit=\"batch\")\n",
    "    \n",
    "    # with torch.no_grad():  # Disable gradient calculations for evaluation\n",
    "    #     for batch_idx, (inputs, targets) in enumerate(progress_bar):\n",
    "    #         inputs, targets = inputs.to(device), targets.to(device)\n",
    "            \n",
    "    #         outputs = model(inputs)\n",
    "    #         loss = criterion(outputs, targets)\n",
    "            \n",
    "    #         running_loss += loss.item()\n",
    "            \n",
    "    #         progress_bar.set_postfix({\n",
    "    #             \"batch_loss\": loss.item(),\n",
    "    #             \"batch_index\": batch_idx + 1,\n",
    "    #             \"batch_size\": inputs.size(0)\n",
    "    #         })\n",
    "    \n",
    "    # # Average loss over all batches\n",
    "    # val_loss = running_loss / len(val_loader)\n",
    "    # print(f\"Val Loss: {val_loss:.4f}\")\n",
    " \n",
    "# Save the trained model\n",
    "torch.save(model.state_dict(), \"image_motion_cnn.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cf36f286-988a-4943-be13-af81a386af0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|████████████████| 1846/1846 [41:01<00:00,  1.33s/batch, batch_loss=1.74, batch_index=1846, batch_size=58]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 776.4066\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# device = (torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "# criterion = torch.nn.MSELoss()\n",
    "# model = NeuralNetwork(device=\"cuda\").to(device)\n",
    "# model.load_state_dict(torch.load(\"image_motion_cnn.pth\", weights_only=True))\n",
    "\n",
    "test_dataset = DoomMotionDataset(coco_test, TEST_RUN)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "running_loss = 0.0\n",
    "\n",
    "\n",
    "progress_bar = tqdm(test_loader, desc=\"Testing\", unit=\"batch\")\n",
    "\n",
    "with torch.no_grad():  # Disable gradient calculations for evaluation\n",
    "    for batch_idx, batch in enumerate(progress_bar):\n",
    "        image, past_motion, targets = batch['image'], batch['past_motion'], batch['target']\n",
    "        image, past_motion, targets = image.to(device), past_motion.to(device), targets.to(device)\n",
    "        \n",
    "        outputs = model(image, past_motion)\n",
    "        loss = criterion(outputs, targets)\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        progress_bar.set_postfix({\n",
    "            \"batch_loss\": loss.item(),\n",
    "            \"batch_index\": batch_idx + 1,\n",
    "            \"batch_size\": image.size(0)\n",
    "        })\n",
    "\n",
    "# Average loss over all batches\n",
    "test_loss = running_loss / len(test_loader)\n",
    "print(f\"Test Loss: {test_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cfa4b278-d5e6-4fe5-a017-e6a02e037978",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'batch_size' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m running_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m      4\u001b[0m val_dataset \u001b[38;5;241m=\u001b[39m DoomMotionDataset(coco_val, VAL_RUN)\n\u001b[1;32m----> 5\u001b[0m val_loader \u001b[38;5;241m=\u001b[39m DataLoader(val_dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[43mbatch_size\u001b[49m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m      7\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m64\u001b[39m\n\u001b[0;32m      9\u001b[0m progress_bar \u001b[38;5;241m=\u001b[39m tqdm(val_loader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation\u001b[39m\u001b[38;5;124m\"\u001b[39m, unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'batch_size' is not defined"
     ]
    }
   ],
   "source": [
    "model.eval()  # Set the model to evaluation mode\n",
    "running_loss = 0.0\n",
    "\n",
    "progress_bar = tqdm(val_loader, desc=\"Validation\", unit=\"batch\")\n",
    "\n",
    "with torch.no_grad():  # Disable gradient calculations for evaluation\n",
    "    for batch_idx, batch in enumerate(progress_bar):\n",
    "        image, past_motion, targets = batch['image'], batch['past_motion'], batch['target']\n",
    "        image, past_motion, targets = image.to(device), past_motion.to(device), targets.to(device)\n",
    "        \n",
    "        outputs = model(image, past_motion)\n",
    "        loss = criterion(outputs, targets)\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        progress_bar.set_postfix({\n",
    "            \"batch_loss\": loss.item(),\n",
    "            \"batch_index\": batch_idx + 1,\n",
    "            \"batch_size\": image.size(0)\n",
    "        })\n",
    "\n",
    "# Average loss over all batches\n",
    "val_loss = running_loss / len(val_loader)\n",
    "print(f\"Val Loss: {val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ff5d9cf8-fa9f-4f0d-bd87-fad53e55fa61",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|███████████| 2972/2972 [1:08:33<00:00,  1.38s/batch, batch_loss=4.48, batch_index=2972, batch_size=46]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 636.3955\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "device = (torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "model = NeuralNetwork(device=\"cuda\").to(device)\n",
    "model.load_state_dict(torch.load(\"image_motion_cnn_8epoch.pth\", weights_only=True))\n",
    "\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "running_loss = 0.0\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "val_dataset = DoomMotionDataset(coco_val, VAL_RUN)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "progress_bar = tqdm(val_loader, desc=\"Validation\", unit=\"batch\")\n",
    "\n",
    "with torch.no_grad():  # Disable gradient calculations for evaluation\n",
    "    for batch_idx, batch in enumerate(progress_bar):\n",
    "        image, past_motion, targets = batch['image'], batch['past_motion'], batch['target']\n",
    "        image, past_motion, targets = image.to(device), past_motion.to(device), targets.to(device)\n",
    "        \n",
    "        outputs = model(image, past_motion)\n",
    "        loss = criterion(outputs, targets)\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        progress_bar.set_postfix({\n",
    "            \"batch_loss\": loss.item(),\n",
    "            \"batch_index\": batch_idx + 1,\n",
    "            \"batch_size\": image.size(0)\n",
    "        })\n",
    "\n",
    "# Average loss over all batches\n",
    "val_loss = running_loss / len(val_loader)\n",
    "print(f\"Val Loss: {val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3c5a48f9-0a71-416d-8fcc-e50d7da2ef59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|████████████████| 1846/1846 [42:28<00:00,  1.38s/batch, batch_loss=4.41, batch_index=1846, batch_size=58]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 772.8054\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_dataset = DoomMotionDataset(coco_test, TEST_RUN)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "running_loss = 0.0\n",
    "\n",
    "\n",
    "progress_bar = tqdm(test_loader, desc=\"Testing\", unit=\"batch\")\n",
    "\n",
    "with torch.no_grad():  # Disable gradient calculations for evaluation\n",
    "    for batch_idx, batch in enumerate(progress_bar):\n",
    "        image, past_motion, targets = batch['image'], batch['past_motion'], batch['target']\n",
    "        image, past_motion, targets = image.to(device), past_motion.to(device), targets.to(device)\n",
    "        \n",
    "        outputs = model(image, past_motion)\n",
    "        loss = criterion(outputs, targets)\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        progress_bar.set_postfix({\n",
    "            \"batch_loss\": loss.item(),\n",
    "            \"batch_index\": batch_idx + 1,\n",
    "            \"batch_size\": image.size(0)\n",
    "        })\n",
    "\n",
    "# Average loss over all batches\n",
    "test_loss = running_loss / len(test_loader)\n",
    "print(f\"Test Loss: {test_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022f1c72-9536-41a0-860a-1a02def70a66",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
