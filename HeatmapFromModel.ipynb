{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c437917-e990-4491-a25e-b7adeb77f372",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "from pycocotools.coco import COCO\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import open3d as o3d\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as functions\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9003254-3d2c-4f84-91a9-0c6d966f104e",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATADIR = \"cocodoom/\"\n",
    "\n",
    "dataSplit, run = \"run-full-test\", \"run3\"\n",
    "\n",
    "annFile = '{}{}.json'.format(DATADIR,dataSplit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8df8e50-550c-43a3-8b8e-b5a5e41ed0b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=13.70s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "coco = COCO(annFile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f78d63-9051-41a4-b518-1a30404c56cd",
   "metadata": {},
   "source": [
    "<h1>Heatmap Code</h1>\n",
    "This code is used to generate the heatmap - the only function you need to use is generateHeatmap, which takes in the most recent depth map and a motion vector - and returns the heatmap and also a point cloud of only the points visible from the new location (which is very helpful for verification).\n",
    "\n",
    "Providing the depth map and motion vector to this function can be done quite easily from the example at the bottom of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e816bf65-8db0-4c71-ba88-9a17b530f7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSegmentationMask(rgb_filename):\n",
    "    return rgb_filename.replace(\"rgb\", \"objects\")\n",
    "\n",
    "def getDepthMask(rgb_filename):\n",
    "    return rgb_filename.replace(\"rgb\", \"depth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8680de85-df84-4334-87be-8defcfc6e6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def toPointCloud(depth):\n",
    "    # https://github.com/mmatl/pyrender/issues/14#issuecomment-485881479 was used as reference\n",
    "    height, width = depth.shape\n",
    "    #print(depth.shape)\n",
    "    fy = 200 / np.tan(1.5708 * 0.5)\n",
    "    fx = 320 / np.tan(1.5708 * 0.5)\n",
    "\n",
    "    depth = depth / 64.0\n",
    "    mask = np.where(depth > 0)\n",
    "\n",
    "    #print(depth.max(axis=1))\n",
    "\n",
    "    x = mask[1]\n",
    "    y = mask[0]\n",
    "\n",
    "    normalized_x = (x.astype(np.float32) - width * 0.5) #/ width\n",
    "    normalized_y = (y.astype(np.float32) - height * 0.5) #/ height\n",
    "    \n",
    "    world_x = normalized_x * depth[y, x] / fx #* 1000\n",
    "    world_y = normalized_y * depth[y, x] / fy #* 1000\n",
    "    world_z = depth[y, x]\n",
    "    ones = np.ones(world_z.shape[0], dtype=np.float32)\n",
    "\n",
    "    return np.vstack((world_x, world_y, world_z)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "03b80b82-8e4b-46ef-bce5-7ac9ae8e26f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visiblePoints(point_cloud, motion_vector):\n",
    "    \"\"\"\n",
    "    Takes in the projected point cloud and motion vector and \n",
    "    generates a heatmap on the original image. Also produces\n",
    "    a point cloud of only the points visible after movement.\n",
    "    \"\"\"\n",
    "    angle = motion_vector[3]\n",
    "    rotation_matrix = np.array([\n",
    "        [np.cos(angle), 0, -1 * np.sin(angle)],[0, 1, 0],[np.sin(angle), 0, np.cos(angle)]\n",
    "        ])\n",
    "    #print(point_cloud.shape)\n",
    "    translated_points = point_cloud - np.array([motion_vector[1], motion_vector[2], motion_vector[0]])\n",
    "    #print(translated_points.shape)\n",
    "    #print(rotation_matrix.shape)\n",
    "    transformed_points = np.zeros(translated_points.shape)\n",
    "    # This has been split up as the kernel kept dying\n",
    "    for i, point in enumerate(translated_points):\n",
    "        transformed_points[i] = rotation_matrix @ point\n",
    "\n",
    "    fy = 200 / np.tan(1.5708 * 0.5)\n",
    "    fx = 320 / np.tan(1.5708 * 0.5)\n",
    "    x_proj = (transformed_points[:, 0] * fx / transformed_points[:, 2]) + 320 * 0.5\n",
    "    y_proj = (transformed_points[:, 1] * fy / transformed_points[:, 2]) + 200 * 0.5\n",
    "\n",
    "    #print(\"Made it here\")\n",
    "\n",
    "    output = np.zeros((200, 320)) # These might need to be switched\n",
    "    visible_point = []\n",
    "    for i, (x, y, z) in enumerate(transformed_points):\n",
    "        if z > 0 and 0 <= x_proj[i] < 320 and 0 <= y_proj[i] < 200:\n",
    "            output[i // 320, i % 320] = 1\n",
    "            visible_point.append(transformed_points[i])\n",
    "\n",
    "    return output, np.array(visible_point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e6b1dbce-44fe-4aba-bdac-6068fdfc4172",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateHeatmap(depth, motion_vector):\n",
    "    \"\"\"\n",
    "    Takes in the path to the depth map and the predicted or \n",
    "    actual motion vector to generate a heatmap.\n",
    "\n",
    "    The path to the depth map should be structured as:\n",
    "    runX/mapXX/depth/XXXXXX.png\n",
    "    \"\"\"\n",
    "\n",
    "    point_cloud = toPointCloud(cv2.imread(DATADIR + getDepthMask(depth), cv2.IMREAD_UNCHANGED))\n",
    "    heatmap, visible_point_cloud = visiblePoints(point_cloud, motion_vector)\n",
    "\n",
    "    return heatmap, visible_point_cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aec930d7-0a74-41b0-82bc-30d5d9243d93",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'img' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m heatmap, visible_point_cloud \u001b[38;5;241m=\u001b[39m generateHeatmap(\u001b[43mimg\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile_name\u001b[39m\u001b[38;5;124m\"\u001b[39m], combined_motion_vectors)\n\u001b[1;32m      2\u001b[0m plt\u001b[38;5;241m.\u001b[39mimsave(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinary_image.png\u001b[39m\u001b[38;5;124m'\u001b[39m, heatmap, cmap\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgray\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpng\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'img' is not defined"
     ]
    }
   ],
   "source": [
    "#heatmap, visible_point_cloud = generateHeatmap(img[\"file_name\"], combined_motion_vectors)\n",
    "#plt.imsave('binary_image.png', heatmap, cmap='gray', format='png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e5fb58-7046-41dc-859c-52e8c40c3eeb",
   "metadata": {},
   "source": [
    "<h1>Dataset</h1>\n",
    "This code converts the various CocoDoom inputs into better formats to use - example usage of this is provided below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4d5625b9-36c7-40bc-8124-3429d0b482e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "player_positions = {\"run1\":[], \"run2\":[], \"run3\":[]}\n",
    "motion_vectors = {\"run1\":[], \"run2\":[], \"run3\":[]}\n",
    "USED_RUNS = [\"run3\"]\n",
    "for run in USED_RUNS:\n",
    "    with open(DATADIR+run+\"/log.txt\", 'r') as log_file:\n",
    "        for line in log_file:\n",
    "            if \"player\" in line:\n",
    "                line = line.strip()\n",
    "                tic, stats = line.split(\"player:\")\n",
    "                x, y, z, angle = stats.split(\",\")\n",
    "    \n",
    "                # Store position in the dictionary\n",
    "                player_positions[run].append([float(x), float(y), float(z), float(angle)])\n",
    "                if len(player_positions[run]) >= 2:\n",
    "                    player_position = player_positions[run][-1]\n",
    "                    prev_player_position = player_positions[run][-2]\n",
    "                    \n",
    "                    dx = player_position[0] - prev_player_position[0]\n",
    "                    dy = player_position[1] - prev_player_position[1]\n",
    "                    dz = player_position[2] - prev_player_position[2]\n",
    "                    dangle = np.pi - abs(abs(player_position[3] - prev_player_position[3]) - np.pi)\n",
    "                    \n",
    "                    dx_relative = dx * np.cos(2 * np.pi - prev_player_position[3]) + dy * np.cos(prev_player_position[3] - 1/2 * np.pi)\n",
    "                    dy_relative = dx * np.sin(2 * np.pi - prev_player_position[3]) + dy * np.sin(prev_player_position[3] - 1/2 * np.pi)\n",
    "                    motion_vector = [dx_relative, dy_relative, dz, dangle]\n",
    "                    motion_vectors[run].append(motion_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8e53bf0a-5d42-4553-acab-db8a4f963e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoomMotionDataset(Dataset):\n",
    "    def __init__(self, coco, run, input_window, prediction_window, transform=None):\n",
    "        self.coco = coco\n",
    "        self.run = run\n",
    "        self.img_ids = self.coco.getImgIds()\n",
    "        self.transform = transform\n",
    "        self.input_window = input_window\n",
    "        self.prediction_window = prediction_window\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_ids)\n",
    "\n",
    "    def fullSegmentationFormat(self, rgb_filename):\n",
    "        seg_image = self.load_image(self.getSegmentationMask(DATADIR + rgb_filename))\n",
    "        if seg_image == None:\n",
    "            return seg_image\n",
    "        seg_class_map = self.color_to_index(seg_image)\n",
    "        seg_class_one_hot = functions.one_hot(seg_class_map, num_classes=4).to(dtype=torch.float).permute(2, 0, 1)\n",
    "        return seg_class_one_hot\n",
    "\n",
    "    def fullDepthFormat(self, rgb_filename):\n",
    "        depth_mask = self.load_image(self.getDepthMask(DATADIR + rgb_filename))\n",
    "        if depth_mask == None:\n",
    "            return depth_mask\n",
    "        depth_mask = torch.tensor(depth_mask, dtype=torch.float32)\n",
    "        return depth_mask\n",
    "\n",
    "    def getSegmentationMask(self, rgb_filename):\n",
    "        return rgb_filename.replace(\"rgb\", \"objects\")\n",
    "\n",
    "    def getDepthMask(self, rgb_filename):\n",
    "        return rgb_filename.replace(\"rgb\", \"depth\")\n",
    "\n",
    "    def color_to_index(self, segmentation_image):\n",
    "        # Map colors to class indices\n",
    "        r, g, b = segmentation_image\n",
    "        pixel_values = r + (g *  2**8) + (b * 2**16)  # From cocodoom documentation, converts to an object id\n",
    "\n",
    "        class_map = torch.full_like(pixel_values, 3, dtype=torch.long)\n",
    "\n",
    "        sky = (1 << 23) + 0\n",
    "        horizontal = (1 << 23) + 1\n",
    "        vertical = (1 << 23) + 2\n",
    "        \n",
    "        class_map[x == sky] = 0\n",
    "        class_map[x == horizontal] = 1\n",
    "        class_map[x == vertical] = 2\n",
    "        return class_map\n",
    "\n",
    "    def load_image(self, path):\n",
    "        if os.path.exists(path):\n",
    "            img = Image.open(path)\n",
    "            return transforms.ToTensor()(img)\n",
    "        return None\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load the RGB image\n",
    "        rgb_filename = self.coco.loadImgs(self.img_ids[idx])[0]['file_name']\n",
    "        #print(rgb_filename)\n",
    "        tic = int(rgb_filename.replace(\".png\", \"\").split(\"/\")[-1])\n",
    "        next_tic = tic+1\n",
    "        previous_tic = tic-1\n",
    "        prev_motion_vectors = []\n",
    "        next_motion_vectors = []\n",
    "        prev_seg = []\n",
    "        prev_dep = []\n",
    "\n",
    "        for t in range(input_window, 0, -1):\n",
    "            if tic-t < 0:\n",
    "                prev_motion_vectors.append(motion_vectors[self.run][0])\n",
    "                prev_filename = self.coco.loadImgs(self.img_ids[0])[0]['file_name']\n",
    "                seg = self.fullSegmentationFormat(prev_filename)\n",
    "                dep = self.fullDepthFormat(prev_filename)\n",
    "                prev_seg.append(seg)\n",
    "                prev_dep.append(dep)\n",
    "                continue\n",
    "            elif tic-t >= len(motion_vectors[self.run]):\n",
    "                prev_motion_vectors.append(motion_vectors[self.run][-1])\n",
    "                prev_filename = self.coco.loadImgs(self.img_ids[-1])[0]['file_name']\n",
    "                seg = self.fullSegmentationFormat(prev_filename)\n",
    "                dep = self.fullDepthFormat(prev_filename)\n",
    "                prev_seg.append(seg)\n",
    "                prev_dep.append(dep)\n",
    "                continue\n",
    "            prev_motion_vectors.append(motion_vectors[self.run][tic-t])\n",
    "            prev_filename = rgb_filename[:-10] + str(max(tic - t, 2)).rjust(6, \"0\") + \".png\"\n",
    "            # run1/map01/rgb/000002.png\n",
    "            if os.path.exists(DATADIR + prev_filename):\n",
    "                seg = self.fullSegmentationFormat(prev_filename)\n",
    "                #print(f\"seg shape: {seg.shape}\")\n",
    "                dep = self.fullDepthFormat(prev_filename)\n",
    "                #print(f\"dep shape: {dep.shape}\")\n",
    "                prev_seg.append(seg)\n",
    "                prev_dep.append(dep)\n",
    "            else:\n",
    "                prev_seg.append(torch.zeros((4, 200, 320)))\n",
    "                prev_dep.append(torch.zeros((1, 200, 320)))\n",
    "                \n",
    "\n",
    "        for t in range(1, prediction_window+1):\n",
    "            if tic+t >= len(motion_vectors[self.run]):\n",
    "                next_motion_vectors.append(motion_vectors[self.run][-1])\n",
    "                continue\n",
    "            next_motion_vectors.append(motion_vectors[self.run][tic+t])\n",
    "\n",
    "            \n",
    "        prev_motion_vectors = torch.tensor(prev_motion_vectors, dtype=torch.float32)\n",
    "        next_motion_vectors = torch.tensor(next_motion_vectors, dtype=torch.float32)\n",
    "        #print(len(prev_seg))\n",
    "        prev_seg = torch.stack(prev_seg)\n",
    "        prev_dep = torch.stack(prev_dep)\n",
    "        \n",
    "        return {\"prev_motion\" : prev_motion_vectors, \"next_motion\" : next_motion_vectors, \"previous_seg\" : prev_seg, \"previous_dep\" : prev_dep}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca6ea78-3a5e-470b-98bc-a571ad0df618",
   "metadata": {},
   "source": [
    "<h1>Model</h1>\n",
    "This is the code that defines the main model to be used. Example usage of this to predict is provided below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f23c9beb-3602-4a0a-97ec-69e383d84cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "  def __init__(self, batch_size, input_length, sequence_length, activation_function=functions.relu, device=torch.device(\"cpu\")):\n",
    "    super(NeuralNetwork, self).__init__()\n",
    "    self.batch_size = batch_size\n",
    "    self.input_length = input_length\n",
    "    self.sequence_length = sequence_length\n",
    "\n",
    "    # Encoder\n",
    "    # Conv layers\n",
    "    self.conv_seg = nn.Conv2d(4, 1, kernel_size=3, stride=2, padding=1, bias=False).to(device)\n",
    "    self.conv_dep = nn.Conv2d(1, 1, kernel_size=3, stride=2, padding=1, bias=False).to(device)\n",
    "\n",
    "    self.motion_fc = nn.Linear(4, 32).to(device)\n",
    "      \n",
    "    # Pre-fusion LSTMs\n",
    "    self.vis_LSTM = nn.LSTM(input_size=32000, hidden_size=256, batch_first=True).to(device)\n",
    "    self.inertia_LSTM = nn.LSTM(input_size=32, hidden_size=256, batch_first=True).to(device)\n",
    "\n",
    "    # Fusion LSTM\n",
    "    self.fusion_LSTM = nn.LSTM(input_size=512, hidden_size=256, batch_first=True).to(device)\n",
    "\n",
    "    # Decoder\n",
    "    self.de_motion_fc = nn.Linear(4, 32).to(device)\n",
    "    self.de_vis_LSTM = nn.LSTM(input_size=32, hidden_size=256, batch_first=True).to(device) #Unsure what the input size of this should be as it actually receives nothing\n",
    "    self.de_inertia_LSTM = nn.LSTM(input_size=32, hidden_size=256, batch_first=True).to(device)\n",
    "    self.de_fusion_LSTM = nn.LSTM(input_size=512, hidden_size=256, batch_first=True).to(device)\n",
    "    self.output_fc = nn.Linear(256, 4).to(device)\n",
    "\n",
    "  def forward(self, segmentation, depth, prev_motion):\n",
    "    hidden_vis = None\n",
    "    hidden_inert = None\n",
    "    hidden_fus = None\n",
    "    \n",
    "    for t in range(self.input_length):\n",
    "        #print(segmentation.shape)\n",
    "        seg = self.conv_seg(segmentation[:,t])\n",
    "        #print(seg.shape)\n",
    "        dep = self.conv_dep(depth[:,t])\n",
    "        #print(dep.shape)\n",
    "        mot = self.motion_fc(prev_motion[:,t])\n",
    "        vis = torch.cat((seg, dep), dim=1)\n",
    "        vis = torch.flatten(vis, start_dim=1)\n",
    "        #print(vis.shape)\n",
    "        if hidden_vis != None:\n",
    "            output_vis, hidden_vis = self.vis_LSTM(vis, hidden_vis)\n",
    "        else:\n",
    "            output_vis, hidden_vis = self.vis_LSTM(vis)\n",
    "        if hidden_inert != None:\n",
    "            output_inert, hidden_inert = self.inertia_LSTM(mot, hidden_inert)\n",
    "        else:\n",
    "            output_inert, hidden_inert = self.inertia_LSTM(mot)\n",
    "        combined = torch.cat((output_vis, output_inert), dim=1)\n",
    "        if hidden_fus != None:\n",
    "            _, hidden_fus = self.fusion_LSTM(combined, hidden_fus)\n",
    "        else:\n",
    "            _, hidden_fus = self.fusion_LSTM(combined)\n",
    "\n",
    "    #print(\"Prev motion: \" + str(prev_motion.shape))\n",
    "    de_mot = prev_motion[:,-1]\n",
    "    output_tensor = torch.zeros(self.sequence_length, segmentation.size(0), 4).to(segmentation.device)\n",
    "    for t in range(self.sequence_length):\n",
    "        #print(de_mot.shape)\n",
    "        de_mot = self.de_motion_fc(de_mot)\n",
    "        de_output_inert, hidden_inert = self.de_inertia_LSTM(de_mot, hidden_inert)\n",
    "        de_output_vis, hidden_vis = self.de_vis_LSTM(torch.zeros(segmentation.size(0), 32).to(segmentation.device), hidden_vis)\n",
    "        #print(de_output_vis.shape, de_output_inert.shape)\n",
    "        combined = torch.cat((de_output_vis, de_output_inert), dim=1)\n",
    "        de_output_fus, hidden_fus = self.de_fusion_LSTM(combined, hidden_fus)\n",
    "        #print(\"de_output_fus: \" + str(de_output_fus.shape))\n",
    "        output_t = self.output_fc(de_output_fus)\n",
    "        #print(\"output_t: \" + str(output_t.shape))\n",
    "        #output_t = output_t.unsqueeze(0)\n",
    "        de_mot = output_t\n",
    "        output_tensor[t] = output_t.unsqueeze(0)\n",
    "        \n",
    "    return output_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817c1d67-a3ad-4039-8f8c-16fa4ba9e16b",
   "metadata": {},
   "source": [
    "<h1>Example loading from the dataset and making predictions</h1>\n",
    "\n",
    "This is an example way to load from the dataset and make predictions from the model.\n",
    "\n",
    "Key parameters here are the input window (how many frames before prediction to give as context to the model), and the prediction window (how many frames into the future to predict).\n",
    "\n",
    "The output of the model is a prediction window amount of motion vectors. The first motion vector in the prediction window represents the motion vector between the last seen frame and the first prediction, the second motion vector in the prediction window represents the motion vector between the last prediction, and the next prediction. This is important to distinguish from each motion vector output representing the difference between the last observed frame and position. Of course calculating this value just consists of adding up the predicted motion vectors up to and including the one we are interested in.\n",
    "\n",
    "Using this summed value of the previous predicted motion vectors needs to be done when generating a heatmap - as otherwise it will not properly represent the predicted movement between the last observed frame.\n",
    "\n",
    "The next_motion variable provides the ground-truth motion vectors, these are structured in the same way as the output of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "506b262f-5e7c-46f6-b557-e32dde6eb14f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1704909/1690540015.py:25: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  depth_mask = torch.tensor(depth_mask, dtype=torch.float32)\n",
      "Testing:   0%| | 158/118138 [00:09<2:01:33, 16.18batch/s, batch_index=158, batch\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m progress_bar \u001b[38;5;241m=\u001b[39m tqdm(test_loader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTesting\u001b[39m\u001b[38;5;124m\"\u001b[39m, unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():  \u001b[38;5;66;03m# Disable gradient calculations for evaluation\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(progress_bar):\n\u001b[1;32m     20\u001b[0m         prev_motion, next_motion, previous_seg, previous_dep \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprev_motion\u001b[39m\u001b[38;5;124m\"\u001b[39m], batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnext_motion\u001b[39m\u001b[38;5;124m\"\u001b[39m], batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprevious_seg\u001b[39m\u001b[38;5;124m\"\u001b[39m], batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprevious_dep\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     21\u001b[0m         prev_motion, next_motion, previous_seg, previous_dep \u001b[38;5;241m=\u001b[39m prev_motion\u001b[38;5;241m.\u001b[39mto(device), next_motion\u001b[38;5;241m.\u001b[39mto(device), previous_seg\u001b[38;5;241m.\u001b[39mto(device), previous_dep\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m/shared/storage/cs/studentscratch/js3921/python_environments/envs/project_env_cuda/lib/python3.9/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m/shared/storage/cs/studentscratch/js3921/python_environments/envs/project_env_cuda/lib/python3.9/site-packages/torch/utils/data/dataloader.py:708\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    707\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 708\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    709\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    710\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    711\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    712\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    713\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    714\u001b[0m ):\n",
      "File \u001b[0;32m/shared/storage/cs/studentscratch/js3921/python_environments/envs/project_env_cuda/lib/python3.9/site-packages/torch/utils/data/dataloader.py:764\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    762\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    763\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 764\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    765\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    766\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/shared/storage/cs/studentscratch/js3921/python_environments/envs/project_env_cuda/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/shared/storage/cs/studentscratch/js3921/python_environments/envs/project_env_cuda/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[10], line 89\u001b[0m, in \u001b[0;36mDoomMotionDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;66;03m# run1/map01/rgb/000002.png\u001b[39;00m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(DATADIR \u001b[38;5;241m+\u001b[39m prev_filename):\n\u001b[0;32m---> 89\u001b[0m     seg \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfullSegmentationFormat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprev_filename\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m     \u001b[38;5;66;03m#print(f\"seg shape: {seg.shape}\")\u001b[39;00m\n\u001b[1;32m     91\u001b[0m     dep \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfullDepthFormat(prev_filename)\n",
      "Cell \u001b[0;32mIn[10], line 14\u001b[0m, in \u001b[0;36mDoomMotionDataset.fullSegmentationFormat\u001b[0;34m(self, rgb_filename)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfullSegmentationFormat\u001b[39m(\u001b[38;5;28mself\u001b[39m, rgb_filename):\n\u001b[0;32m---> 14\u001b[0m     seg_image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_image\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetSegmentationMask\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDATADIR\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mrgb_filename\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m seg_image \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     16\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m seg_image\n",
      "Cell \u001b[0;32mIn[10], line 53\u001b[0m, in \u001b[0;36mDoomMotionDataset.load_image\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(path):\n\u001b[1;32m     52\u001b[0m     img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(path)\n\u001b[0;32m---> 53\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtransforms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mToTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/shared/storage/cs/studentscratch/js3921/python_environments/envs/project_env_cuda/lib/python3.9/site-packages/torchvision/transforms/transforms.py:137\u001b[0m, in \u001b[0;36mToTensor.__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pic):\n\u001b[1;32m    130\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;124;03m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;124;03m        Tensor: Converted image.\u001b[39;00m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/shared/storage/cs/studentscratch/js3921/python_environments/envs/project_env_cuda/lib/python3.9/site-packages/torchvision/transforms/functional.py:168\u001b[0m, in \u001b[0;36mto_tensor\u001b[0;34m(pic)\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;66;03m# handle PIL Image\u001b[39;00m\n\u001b[1;32m    167\u001b[0m mode_to_nptype \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI\u001b[39m\u001b[38;5;124m\"\u001b[39m: np\u001b[38;5;241m.\u001b[39mint32, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI;16\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m sys\u001b[38;5;241m.\u001b[39mbyteorder \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlittle\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI;16B\u001b[39m\u001b[38;5;124m\"\u001b[39m: np\u001b[38;5;241m.\u001b[39mint16, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mF\u001b[39m\u001b[38;5;124m\"\u001b[39m: np\u001b[38;5;241m.\u001b[39mfloat32}\n\u001b[0;32m--> 168\u001b[0m img \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode_to_nptype\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muint8\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m)\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pic\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    171\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m255\u001b[39m \u001b[38;5;241m*\u001b[39m img\n",
      "File \u001b[0;32m/shared/storage/cs/studentscratch/js3921/python_environments/envs/project_env_cuda/lib/python3.9/site-packages/PIL/Image.py:747\u001b[0m, in \u001b[0;36mImage.__array_interface__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    745\u001b[0m     new[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtobytes(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mL\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    746\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 747\u001b[0m     new[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtobytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    748\u001b[0m new[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshape\u001b[39m\u001b[38;5;124m\"\u001b[39m], new[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtypestr\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m _conv_type_shape(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    749\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m new\n",
      "File \u001b[0;32m/shared/storage/cs/studentscratch/js3921/python_environments/envs/project_env_cuda/lib/python3.9/site-packages/PIL/Image.py:796\u001b[0m, in \u001b[0;36mImage.tobytes\u001b[0;34m(self, encoder_name, *args)\u001b[0m\n\u001b[1;32m    793\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m encoder_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m encoder_args \u001b[38;5;241m==\u001b[39m ():\n\u001b[1;32m    794\u001b[0m     encoder_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode\n\u001b[0;32m--> 796\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwidth \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheight \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    799\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m/shared/storage/cs/studentscratch/js3921/python_environments/envs/project_env_cuda/lib/python3.9/site-packages/PIL/ImageFile.py:300\u001b[0m, in \u001b[0;36mImageFile.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    297\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(msg)\n\u001b[1;32m    299\u001b[0m b \u001b[38;5;241m=\u001b[39m b \u001b[38;5;241m+\u001b[39m s\n\u001b[0;32m--> 300\u001b[0m n, err_code \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    301\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    302\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_size = 1\n",
    "input_window = 5\n",
    "prediction_window = 10\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = NeuralNetwork(batch_size, input_window, prediction_window, device=device).to(device)\n",
    "model.load_state_dict(torch.load(\"multimodal_seq2seq.pth\", weights_only=True))\n",
    "\n",
    "test_dataset = DoomMotionDataset(coco, run, input_window, prediction_window)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "progress_bar = tqdm(test_loader, desc=\"Testing\", unit=\"batch\")\n",
    "\n",
    "with torch.no_grad():  # Disable gradient calculations for evaluation\n",
    "    for batch_idx, batch in enumerate(progress_bar):\n",
    "        prev_motion, next_motion, previous_seg, previous_dep = batch[\"prev_motion\"], batch[\"next_motion\"], batch[\"previous_seg\"], batch[\"previous_dep\"]\n",
    "        prev_motion, next_motion, previous_seg, previous_dep = prev_motion.to(device), next_motion.to(device), previous_seg.to(device), previous_dep.to(device)\n",
    "\n",
    "        if prev_motion.size(0) != next_motion.size(0) != previous_seg.size(0) != previous_dep.size(0):\n",
    "                continue\n",
    "            \n",
    "        outputs = model(previous_seg, previous_dep, prev_motion)\n",
    "        outputs = outputs.permute(1, 0, 2)\n",
    "\n",
    "        depth_map = previous_dep[0][-1]\n",
    "\n",
    "        # Generate heatmaps here\n",
    "        \n",
    "        progress_bar.set_postfix({\n",
    "            \"batch_index\": batch_idx + 1,\n",
    "            \"batch_size\": prev_motion.size(0)\n",
    "        })"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
