{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c437917-e990-4491-a25e-b7adeb77f372",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "from pycocotools.coco import COCO\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import open3d as o3d\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as functions\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9003254-3d2c-4f84-91a9-0c6d966f104e",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATADIR = \"cocodoom/\"\n",
    "\n",
    "dataSplit, run = \"run-full-test\", \"run3\"\n",
    "\n",
    "annFile = '{}{}.json'.format(DATADIR,dataSplit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8df8e50-550c-43a3-8b8e-b5a5e41ed0b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=16.59s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done (t=9.82s)\n",
      "creating index...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index created!\n"
     ]
    }
   ],
   "source": [
    "coco = COCO(annFile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f78d63-9051-41a4-b518-1a30404c56cd",
   "metadata": {},
   "source": [
    "<h1>Heatmap Code</h1>\n",
    "This code is used to generate the heatmap - the only function you need to use is generateHeatmap, which takes in the most recent depth map and a motion vector - and returns the heatmap and also a point cloud of only the points visible from the new location (which is very helpful for verification).\n",
    "\n",
    "Providing the depth map and motion vector to this function can be done quite easily from the example at the bottom of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e816bf65-8db0-4c71-ba88-9a17b530f7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSegmentationMask(rgb_filename):\n",
    "    return rgb_filename.replace(\"rgb\", \"objects\")\n",
    "\n",
    "def getDepthMask(rgb_filename):\n",
    "    return rgb_filename.replace(\"rgb\", \"depth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8680de85-df84-4334-87be-8defcfc6e6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def toPointCloud(depth):\n",
    "    # https://github.com/mmatl/pyrender/issues/14#issuecomment-485881479 was used as reference\n",
    "    height, width = depth.shape\n",
    "    #print(depth.shape)\n",
    "    fy = 200 / np.tan(1.5708 * 0.5)\n",
    "    fx = 320 / np.tan(1.5708 * 0.5)\n",
    "\n",
    "    depth = depth / 64.0\n",
    "    mask = np.where(depth > 0)\n",
    "\n",
    "    #print(depth.max(axis=1))\n",
    "\n",
    "    x = mask[1]\n",
    "    y = mask[0]\n",
    "\n",
    "    normalized_x = (x.astype(np.float32) - width * 0.5) #/ width\n",
    "    normalized_y = (y.astype(np.float32) - height * 0.5) #/ height\n",
    "    \n",
    "    world_x = normalized_x * depth[y, x] / fx #* 1000\n",
    "    world_y = normalized_y * depth[y, x] / fy #* 1000\n",
    "    world_z = depth[y, x]\n",
    "    ones = np.ones(world_z.shape[0], dtype=np.float32)\n",
    "\n",
    "    return np.vstack((world_x, world_y, world_z)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "03b80b82-8e4b-46ef-bce5-7ac9ae8e26f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visiblePoints(point_cloud, motion_vector):\n",
    "    \"\"\"\n",
    "    Takes in the projected point cloud and motion vector and \n",
    "    generates a heatmap on the original image. Also produces\n",
    "    a point cloud of only the points visible after movement.\n",
    "    \"\"\"\n",
    "    angle = motion_vector[3]\n",
    "    rotation_matrix = np.array([\n",
    "        [np.cos(angle), 0, -1 * np.sin(angle)],[0, 1, 0],[np.sin(angle), 0, np.cos(angle)]\n",
    "        ])\n",
    "    #print(point_cloud.shape)\n",
    "    translated_points = point_cloud - np.array([motion_vector[1], motion_vector[2], motion_vector[0]])\n",
    "    #print(translated_points.shape)\n",
    "    #print(rotation_matrix.shape)\n",
    "    transformed_points = np.zeros(translated_points.shape)\n",
    "    # This has been split up as the kernel kept dying\n",
    "    for i, point in enumerate(translated_points):\n",
    "        transformed_points[i] = rotation_matrix @ point\n",
    "\n",
    "    fy = 200 / np.tan(1.5708 * 0.5)\n",
    "    fx = 320 / np.tan(1.5708 * 0.5)\n",
    "    x_proj = (transformed_points[:, 0] * fx / transformed_points[:, 2]) + 320 * 0.5\n",
    "    y_proj = (transformed_points[:, 1] * fy / transformed_points[:, 2]) + 200 * 0.5\n",
    "\n",
    "    #print(\"Made it here\")\n",
    "\n",
    "    output = np.zeros((200, 320)) # These might need to be switched\n",
    "    visible_point = []\n",
    "    for i, (x, y, z) in enumerate(transformed_points):\n",
    "        if z > 0 and 0 <= x_proj[i] < 320 and 0 <= y_proj[i] < 200:\n",
    "            output[i // 320, i % 320] = 1\n",
    "            visible_point.append(transformed_points[i])\n",
    "\n",
    "    return output, np.array(visible_point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e6b1dbce-44fe-4aba-bdac-6068fdfc4172",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateHeatmap(depth, motion_vector):\n",
    "    \"\"\"\n",
    "    Takes in the path to the depth map and the predicted or \n",
    "    actual motion vector to generate a heatmap.\n",
    "\n",
    "    The path to the depth map should be structured as:\n",
    "    runX/mapXX/depth/XXXXXX.png\n",
    "    \"\"\"\n",
    "\n",
    "    point_cloud = toPointCloud(cv2.imread(DATADIR + getDepthMask(depth), cv2.IMREAD_UNCHANGED))\n",
    "    heatmap, visible_point_cloud = visiblePoints(point_cloud, motion_vector)\n",
    "\n",
    "    return heatmap, visible_point_cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aec930d7-0a74-41b0-82bc-30d5d9243d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "#heatmap, visible_point_cloud = generateHeatmap(img[\"file_name\"], combined_motion_vectors)\n",
    "#plt.imsave('binary_image.png', heatmap, cmap='gray', format='png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e5fb58-7046-41dc-859c-52e8c40c3eeb",
   "metadata": {},
   "source": [
    "<h1>Dataset</h1>\n",
    "This code converts the various CocoDoom inputs into better formats to use - example usage of this is provided below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4d5625b9-36c7-40bc-8124-3429d0b482e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "player_positions = {\"run1\":[], \"run2\":[], \"run3\":[]}\n",
    "motion_vectors = {\"run1\":[], \"run2\":[], \"run3\":[]}\n",
    "USED_RUNS = [\"run3\"]\n",
    "for run in USED_RUNS:\n",
    "    with open(DATADIR+run+\"/log.txt\", 'r') as log_file:\n",
    "        for line in log_file:\n",
    "            if \"player\" in line:\n",
    "                line = line.strip()\n",
    "                tic, stats = line.split(\"player:\")\n",
    "                x, y, z, angle = stats.split(\",\")\n",
    "    \n",
    "                # Store position in the dictionary\n",
    "                player_positions[run].append([float(x), float(y), float(z), float(angle)])\n",
    "                if len(player_positions[run]) >= 2:\n",
    "                    player_position = player_positions[run][-1]\n",
    "                    prev_player_position = player_positions[run][-2]\n",
    "                    \n",
    "                    dx = player_position[0] - prev_player_position[0]\n",
    "                    dy = player_position[1] - prev_player_position[1]\n",
    "                    dz = player_position[2] - prev_player_position[2]\n",
    "                    dangle = np.pi - abs(abs(player_position[3] - prev_player_position[3]) - np.pi)\n",
    "                    \n",
    "                    dx_relative = dx * np.cos(2 * np.pi - prev_player_position[3]) + dy * np.cos(prev_player_position[3] - 1/2 * np.pi)\n",
    "                    dy_relative = dx * np.sin(2 * np.pi - prev_player_position[3]) + dy * np.sin(prev_player_position[3] - 1/2 * np.pi)\n",
    "                    motion_vector = [dx_relative, dy_relative, dz, dangle]\n",
    "                    motion_vectors[run].append(motion_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8e53bf0a-5d42-4553-acab-db8a4f963e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoomMotionDataset(Dataset):\n",
    "    def __init__(self, coco, run, input_window, prediction_window, transform=None):\n",
    "        self.coco = coco\n",
    "        self.run = run\n",
    "        self.img_ids = self.coco.getImgIds()\n",
    "        self.transform = transform\n",
    "        self.input_window = input_window\n",
    "        self.prediction_window = prediction_window\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_ids)\n",
    "\n",
    "    def fullSegmentationFormat(self, rgb_filename):\n",
    "        seg_image = self.load_image(self.getSegmentationMask(DATADIR + rgb_filename))\n",
    "        if seg_image == None:\n",
    "            return seg_image\n",
    "        seg_class_map = self.color_to_index(seg_image)\n",
    "        seg_class_one_hot = functions.one_hot(seg_class_map, num_classes=4).to(dtype=torch.float).permute(2, 0, 1)\n",
    "        return seg_class_one_hot\n",
    "\n",
    "    def fullDepthFormat(self, rgb_filename):\n",
    "        depth_mask = self.load_image(self.getDepthMask(DATADIR + rgb_filename))\n",
    "        if depth_mask == None:\n",
    "            return depth_mask\n",
    "        depth_mask = torch.tensor(depth_mask, dtype=torch.float32)\n",
    "        return depth_mask\n",
    "\n",
    "    def getSegmentationMask(self, rgb_filename):\n",
    "        return rgb_filename.replace(\"rgb\", \"objects\")\n",
    "\n",
    "    def getDepthMask(self, rgb_filename):\n",
    "        return rgb_filename.replace(\"rgb\", \"depth\")\n",
    "\n",
    "    def color_to_index(self, segmentation_image):\n",
    "        # Map colors to class indices\n",
    "        r, g, b = segmentation_image\n",
    "        pixel_values = r + (g *  2**8) + (b * 2**16)  # From cocodoom documentation, converts to an object id\n",
    "\n",
    "        class_map = torch.full_like(pixel_values, 3, dtype=torch.long)\n",
    "\n",
    "        sky = (1 << 23) + 0\n",
    "        horizontal = (1 << 23) + 1\n",
    "        vertical = (1 << 23) + 2\n",
    "        \n",
    "        class_map[x == sky] = 0\n",
    "        class_map[x == horizontal] = 1\n",
    "        class_map[x == vertical] = 2\n",
    "        return class_map\n",
    "\n",
    "    def load_image(self, path):\n",
    "        if os.path.exists(path):\n",
    "            img = Image.open(path)\n",
    "            return transforms.ToTensor()(img)\n",
    "        return None\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load the RGB image\n",
    "        rgb_filename = self.coco.loadImgs(self.img_ids[idx])[0]['file_name']\n",
    "        #print(rgb_filename)\n",
    "        tic = int(rgb_filename.replace(\".png\", \"\").split(\"/\")[-1])\n",
    "        next_tic = tic+1\n",
    "        previous_tic = tic-1\n",
    "        prev_motion_vectors = []\n",
    "        next_motion_vectors = []\n",
    "        prev_seg = []\n",
    "        prev_dep = []\n",
    "\n",
    "        for t in range(input_window, 0, -1):\n",
    "            if tic-t < 0:\n",
    "                prev_motion_vectors.append(motion_vectors[self.run][0])\n",
    "                prev_filename = self.coco.loadImgs(self.img_ids[0])[0]['file_name']\n",
    "                seg = self.fullSegmentationFormat(prev_filename)\n",
    "                dep = self.fullDepthFormat(prev_filename)\n",
    "                prev_seg.append(seg)\n",
    "                prev_dep.append(dep)\n",
    "                continue\n",
    "            elif tic-t >= len(motion_vectors[self.run]):\n",
    "                prev_motion_vectors.append(motion_vectors[self.run][-1])\n",
    "                prev_filename = self.coco.loadImgs(self.img_ids[-1])[0]['file_name']\n",
    "                seg = self.fullSegmentationFormat(prev_filename)\n",
    "                dep = self.fullDepthFormat(prev_filename)\n",
    "                prev_seg.append(seg)\n",
    "                prev_dep.append(dep)\n",
    "                continue\n",
    "            prev_motion_vectors.append(motion_vectors[self.run][tic-t])\n",
    "            prev_filename = rgb_filename[:-10] + str(max(tic - t, 2)).rjust(6, \"0\") + \".png\"\n",
    "            # run1/map01/rgb/000002.png\n",
    "            if os.path.exists(DATADIR + prev_filename):\n",
    "                seg = self.fullSegmentationFormat(prev_filename)\n",
    "                #print(f\"seg shape: {seg.shape}\")\n",
    "                dep = self.fullDepthFormat(prev_filename)\n",
    "                #print(f\"dep shape: {dep.shape}\")\n",
    "                prev_seg.append(seg)\n",
    "                prev_dep.append(dep)\n",
    "            else:\n",
    "                prev_seg.append(torch.zeros((4, 200, 320)))\n",
    "                prev_dep.append(torch.zeros((1, 200, 320)))\n",
    "                \n",
    "\n",
    "        for t in range(1, prediction_window+1):\n",
    "            if tic+t >= len(motion_vectors[self.run]):\n",
    "                next_motion_vectors.append(motion_vectors[self.run][-1])\n",
    "                continue\n",
    "            next_motion_vectors.append(motion_vectors[self.run][tic+t])\n",
    "\n",
    "            \n",
    "        prev_motion_vectors = torch.tensor(prev_motion_vectors, dtype=torch.float32)\n",
    "        next_motion_vectors = torch.tensor(next_motion_vectors, dtype=torch.float32)\n",
    "        #print(len(prev_seg))\n",
    "        prev_seg = torch.stack(prev_seg)\n",
    "        prev_dep = torch.stack(prev_dep)\n",
    "        \n",
    "        return {\"prev_motion\" : prev_motion_vectors, \"next_motion\" : next_motion_vectors, \"previous_seg\" : prev_seg, \"previous_dep\" : prev_dep, \"file_path\" : rgb_filename}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca6ea78-3a5e-470b-98bc-a571ad0df618",
   "metadata": {},
   "source": [
    "<h1>Model</h1>\n",
    "This is the code that defines the main model to be used. Example usage of this to predict is provided below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f23c9beb-3602-4a0a-97ec-69e383d84cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "  def __init__(self, batch_size, input_length, sequence_length, activation_function=functions.relu, device=torch.device(\"cpu\")):\n",
    "    super(NeuralNetwork, self).__init__()\n",
    "    self.batch_size = batch_size\n",
    "    self.input_length = input_length\n",
    "    self.sequence_length = sequence_length\n",
    "\n",
    "    # Encoder\n",
    "    # Conv layers\n",
    "    self.conv_seg = nn.Conv2d(4, 1, kernel_size=3, stride=2, padding=1, bias=False).to(device)\n",
    "    self.conv_dep = nn.Conv2d(1, 1, kernel_size=3, stride=2, padding=1, bias=False).to(device)\n",
    "\n",
    "    self.motion_fc = nn.Linear(4, 32).to(device)\n",
    "      \n",
    "    # Pre-fusion LSTMs\n",
    "    self.vis_LSTM = nn.LSTM(input_size=32000, hidden_size=256, batch_first=True).to(device)\n",
    "    self.inertia_LSTM = nn.LSTM(input_size=32, hidden_size=256, batch_first=True).to(device)\n",
    "\n",
    "    # Fusion LSTM\n",
    "    self.fusion_LSTM = nn.LSTM(input_size=512, hidden_size=256, batch_first=True).to(device)\n",
    "\n",
    "    # Decoder\n",
    "    self.de_motion_fc = nn.Linear(4, 32).to(device)\n",
    "    self.de_vis_LSTM = nn.LSTM(input_size=32, hidden_size=256, batch_first=True).to(device) #Unsure what the input size of this should be as it actually receives nothing\n",
    "    self.de_inertia_LSTM = nn.LSTM(input_size=32, hidden_size=256, batch_first=True).to(device)\n",
    "    self.de_fusion_LSTM = nn.LSTM(input_size=512, hidden_size=256, batch_first=True).to(device)\n",
    "    self.output_fc = nn.Linear(256, 4).to(device)\n",
    "\n",
    "  def forward(self, segmentation, depth, prev_motion):\n",
    "    hidden_vis = None\n",
    "    hidden_inert = None\n",
    "    hidden_fus = None\n",
    "    \n",
    "    for t in range(self.input_length):\n",
    "        #print(segmentation.shape)\n",
    "        seg = self.conv_seg(segmentation[:,t])\n",
    "        #print(seg.shape)\n",
    "        dep = self.conv_dep(depth[:,t])\n",
    "        #print(dep.shape)\n",
    "        mot = self.motion_fc(prev_motion[:,t])\n",
    "        vis = torch.cat((seg, dep), dim=1)\n",
    "        vis = torch.flatten(vis, start_dim=1)\n",
    "        #print(vis.shape)\n",
    "        if hidden_vis != None:\n",
    "            output_vis, hidden_vis = self.vis_LSTM(vis, hidden_vis)\n",
    "        else:\n",
    "            output_vis, hidden_vis = self.vis_LSTM(vis)\n",
    "        if hidden_inert != None:\n",
    "            output_inert, hidden_inert = self.inertia_LSTM(mot, hidden_inert)\n",
    "        else:\n",
    "            output_inert, hidden_inert = self.inertia_LSTM(mot)\n",
    "        combined = torch.cat((output_vis, output_inert), dim=1)\n",
    "        if hidden_fus != None:\n",
    "            _, hidden_fus = self.fusion_LSTM(combined, hidden_fus)\n",
    "        else:\n",
    "            _, hidden_fus = self.fusion_LSTM(combined)\n",
    "\n",
    "    #print(\"Prev motion: \" + str(prev_motion.shape))\n",
    "    de_mot = prev_motion[:,-1]\n",
    "    output_tensor = torch.zeros(self.sequence_length, segmentation.size(0), 4).to(segmentation.device)\n",
    "    for t in range(self.sequence_length):\n",
    "        #print(de_mot.shape)\n",
    "        de_mot = self.de_motion_fc(de_mot)\n",
    "        de_output_inert, hidden_inert = self.de_inertia_LSTM(de_mot, hidden_inert)\n",
    "        de_output_vis, hidden_vis = self.de_vis_LSTM(torch.zeros(segmentation.size(0), 32).to(segmentation.device), hidden_vis)\n",
    "        #print(de_output_vis.shape, de_output_inert.shape)\n",
    "        combined = torch.cat((de_output_vis, de_output_inert), dim=1)\n",
    "        de_output_fus, hidden_fus = self.de_fusion_LSTM(combined, hidden_fus)\n",
    "        #print(\"de_output_fus: \" + str(de_output_fus.shape))\n",
    "        output_t = self.output_fc(de_output_fus)\n",
    "        #print(\"output_t: \" + str(output_t.shape))\n",
    "        #output_t = output_t.unsqueeze(0)\n",
    "        de_mot = output_t\n",
    "        output_tensor[t] = output_t.unsqueeze(0)\n",
    "        \n",
    "    return output_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817c1d67-a3ad-4039-8f8c-16fa4ba9e16b",
   "metadata": {},
   "source": [
    "<h1>Example loading from the dataset and making predictions</h1>\n",
    "\n",
    "This is an example way to load from the dataset and make predictions from the model.\n",
    "\n",
    "Key parameters here are the input window (how many frames before prediction to give as context to the model), and the prediction window (how many frames into the future to predict).\n",
    "\n",
    "The output of the model is a prediction window amount of motion vectors. The first motion vector in the prediction window represents the motion vector between the last seen frame and the first prediction, the second motion vector in the prediction window represents the motion vector between the last prediction, and the next prediction. This is important to distinguish from each motion vector output representing the difference between the last observed frame and position. Of course calculating this value just consists of adding up the predicted motion vectors up to and including the one we are interested in.\n",
    "\n",
    "Using this summed value of the previous predicted motion vectors needs to be done when generating a heatmap - as otherwise it will not properly represent the predicted movement between the last observed frame.\n",
    "\n",
    "The next_motion variable provides the ground-truth motion vectors, these are structured in the same way as the output of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "506b262f-5e7c-46f6-b557-e32dde6eb14f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3664062/1426843054.py:25: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).?, ?batch/s]\n",
      "  depth_mask = torch.tensor(depth_mask, dtype=torch.float32)\n",
      "Testing:   0%|                                                                                                                                                                                                            | 0/118138 [00:02<?, ?batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24.25047752380371\n",
      "Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1918.1556\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor(1892.7173, device='cuda:0'), tensor(1896.3646, device='cuda:0'), tensor(1900.1072, device='cuda:0'), tensor(1904.2601, device='cuda:0'), tensor(1907.5897, device='cuda:0'), tensor(1910.6614, device='cuda:0'), tensor(1913.7996, device='cuda:0'), tensor(1916.1907, device='cuda:0'), tensor(1918.3767, device='cuda:0'), tensor(1920.4690, device='cuda:0'), tensor(1922.5078, device='cuda:0'), tensor(1924.5469, device='cuda:0'), tensor(1925.6019, device='cuda:0'), tensor(1926.9579, device='cuda:0'), tensor(1927.9398, device='cuda:0'), tensor(1928.9407, device='cuda:0'), tensor(1929.6233, device='cuda:0'), tensor(1931.0037, device='cuda:0'), tensor(1930.6196, device='cuda:0'), tensor(1931.4796, device='cuda:0')]\n",
      "[[tensor(10.3561, device='cuda:0'), tensor(10.8817, device='cuda:0'), tensor(4.2932, device='cuda:0'), tensor(60.0688, device='cuda:0')], [tensor(11.0209, device='cuda:0'), tensor(12.2372, device='cuda:0'), tensor(4.2879, device='cuda:0'), tensor(33.8969, device='cuda:0')], [tensor(12.0283, device='cuda:0'), tensor(13.1234, device='cuda:0'), tensor(4.2373, device='cuda:0'), tensor(27.8612, device='cuda:0')], [tensor(12.9139, device='cuda:0'), tensor(13.7436, device='cuda:0'), tensor(4.2515, device='cuda:0'), tensor(24.5769, device='cuda:0')], [tensor(13.6713, device='cuda:0'), tensor(14.3448, device='cuda:0'), tensor(4.2376, device='cuda:0'), tensor(28.5717, device='cuda:0')], [tensor(14.3304, device='cuda:0'), tensor(14.8995, device='cuda:0'), tensor(4.2127, device='cuda:0'), tensor(38.8901, device='cuda:0')], [tensor(14.9167, device='cuda:0'), tensor(15.4018, device='cuda:0'), tensor(4.1947, device='cuda:0'), tensor(44.4708, device='cuda:0')], [tensor(15.4293, device='cuda:0'), tensor(15.8700, device='cuda:0'), tensor(4.1864, device='cuda:0'), tensor(47.0572, device='cuda:0')], [tensor(15.8918, device='cuda:0'), tensor(16.3139, device='cuda:0'), tensor(4.1833, device='cuda:0'), tensor(48.0548, device='cuda:0')], [tensor(16.2999, device='cuda:0'), tensor(16.7202, device='cuda:0'), tensor(4.1823, device='cuda:0'), tensor(48.2647, device='cuda:0')], [tensor(16.6491, device='cuda:0'), tensor(17.0764, device='cuda:0'), tensor(4.1824, device='cuda:0'), tensor(48.1682, device='cuda:0')], [tensor(16.9683, device='cuda:0'), tensor(17.4082, device='cuda:0'), tensor(4.1804, device='cuda:0'), tensor(48.0274, device='cuda:0')], [tensor(17.2346, device='cuda:0'), tensor(17.7084, device='cuda:0'), tensor(4.1769, device='cuda:0'), tensor(47.8895, device='cuda:0')], [tensor(17.4609, device='cuda:0'), tensor(17.9763, device='cuda:0'), tensor(4.1733, device='cuda:0'), tensor(47.7388, device='cuda:0')], [tensor(17.6667, device='cuda:0'), tensor(18.2236, device='cuda:0'), tensor(4.1713, device='cuda:0'), tensor(47.6270, device='cuda:0')], [tensor(17.8612, device='cuda:0'), tensor(18.4550, device='cuda:0'), tensor(4.1696, device='cuda:0'), tensor(47.5588, device='cuda:0')], [tensor(18.0412, device='cuda:0'), tensor(18.6649, device='cuda:0'), tensor(4.1672, device='cuda:0'), tensor(47.4933, device='cuda:0')], [tensor(18.1950, device='cuda:0'), tensor(18.8567, device='cuda:0'), tensor(4.1659, device='cuda:0'), tensor(47.3864, device='cuda:0')], [tensor(18.3325, device='cuda:0'), tensor(19.0306, device='cuda:0'), tensor(4.1649, device='cuda:0'), tensor(47.3048, device='cuda:0')], [tensor(18.4477, device='cuda:0'), tensor(19.1842, device='cuda:0'), tensor(4.1643, device='cuda:0'), tensor(47.2312, device='cuda:0')]]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1\n",
    "input_window = 5\n",
    "prediction_window = 20\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "model = NeuralNetwork(batch_size, input_window, prediction_window, device=device).to(device)\n",
    "model.load_state_dict(torch.load(\"multimodal_seq2seq_20epochs.pth\", weights_only=True))\n",
    "\n",
    "test_dataset = DoomMotionDataset(coco, run, input_window, prediction_window)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "progress_bar = tqdm(test_loader, desc=\"Testing\", unit=\"batch\")\n",
    "\n",
    "frame_loss = [0] * prediction_window\n",
    "frame_accuracy = [[0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0]]\n",
    "#print(len(frame_loss))\n",
    "\n",
    "running_loss = 0\n",
    "\n",
    "def measure_avg_time_cuda(previous_seg, previous_dep, prev_motion, iterations=100):\n",
    "    model.eval()\n",
    "    starter, ender = torch.cuda.Event(enable_timing=True), torch.cuda.Event(enable_timing=True)\n",
    "    total_time = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(iterations):\n",
    "            starter.record()\n",
    "            _ = model(previous_seg, previous_dep, prev_motion)\n",
    "            ender.record()\n",
    "            torch.cuda.synchronize()  # Ensures accurate timing measurement\n",
    "            total_time += starter.elapsed_time(ender)\n",
    "\n",
    "    return total_time / iterations  # Average time in milliseconds\n",
    "\n",
    "\n",
    "with torch.no_grad():  # Disable gradient calculations for evaluation\n",
    "    for batch_idx, batch in enumerate(progress_bar):\n",
    "        prev_motion, next_motion, previous_seg, previous_dep, file_path = batch[\"prev_motion\"], batch[\"next_motion\"], batch[\"previous_seg\"], batch[\"previous_dep\"], batch[\"file_path\"]\n",
    "        prev_motion, next_motion, previous_seg, previous_dep = prev_motion.to(device), next_motion.to(device), previous_seg.to(device), previous_dep.to(device)\n",
    "        #print(file_path)\n",
    "        if prev_motion.size(0) != next_motion.size(0) != previous_seg.size(0) != previous_dep.size(0):\n",
    "                continue\n",
    "        avg_time_model1 = measure_avg_time_cuda(previous_seg, previous_dep, prev_motion)\n",
    "        break\n",
    "print(avg_time_model1)\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "611cff56-9bd8-4aa1-a4fa-0d86892286f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average inference time Model 1: 24.250478 ms\n"
     ]
    }
   ],
   "source": [
    "print(f\"Average inference time Model 1: {avg_time_model1:.6f} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d277756-1708-48d4-a74c-830daa66c83f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
