{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "caac3c60-cbd9-4794-a8c0-ced5b193b331",
   "metadata": {},
   "source": [
    "<h1>NN Model</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c437917-e990-4491-a25e-b7adeb77f372",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-14T12:34:07.675545Z",
     "iopub.status.busy": "2025-03-14T12:34:07.674675Z",
     "iopub.status.idle": "2025-03-14T12:34:19.887090Z",
     "shell.execute_reply": "2025-03-14T12:34:19.886214Z"
    }
   },
   "outputs": [],
   "source": [
    "from pycocotools.coco import COCO\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as functions\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "#import torchvision\n",
    "from torchvision import transforms\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "edca0610-7a84-420b-81d6-9d3c6063539a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-14T12:34:19.891335Z",
     "iopub.status.busy": "2025-03-14T12:34:19.891078Z",
     "iopub.status.idle": "2025-03-14T12:34:19.924699Z",
     "shell.execute_reply": "2025-03-14T12:34:19.923994Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "print(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9003254-3d2c-4f84-91a9-0c6d966f104e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-14T12:34:19.928497Z",
     "iopub.status.busy": "2025-03-14T12:34:19.928291Z",
     "iopub.status.idle": "2025-03-14T12:34:19.932492Z",
     "shell.execute_reply": "2025-03-14T12:34:19.931505Z"
    }
   },
   "outputs": [],
   "source": [
    "DATADIR = \"cocodoom/\"\n",
    "USED_RUNS = [\"run1\", \"run2\", \"run3\"]\n",
    "\n",
    "dataSplit, TRAIN_RUN = \"run-full-train\", \"run1\"\n",
    "\n",
    "annFile = '{}{}.json'.format(DATADIR,dataSplit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8df8e50-550c-43a3-8b8e-b5a5e41ed0b6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-14T12:34:19.935204Z",
     "iopub.status.busy": "2025-03-14T12:34:19.934909Z",
     "iopub.status.idle": "2025-03-14T12:34:39.790131Z",
     "shell.execute_reply": "2025-03-14T12:34:39.789270Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done (t=18.74s)\n",
      "creating index...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index created!\n"
     ]
    }
   ],
   "source": [
    "coco_train = COCO(annFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eeebf160-b14d-425d-bdc9-48f1f985af94",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-14T12:34:39.794563Z",
     "iopub.status.busy": "2025-03-14T12:34:39.794360Z",
     "iopub.status.idle": "2025-03-14T12:34:39.798334Z",
     "shell.execute_reply": "2025-03-14T12:34:39.797510Z"
    }
   },
   "outputs": [],
   "source": [
    "dataSplit, VAL_RUN = \"run-full-val\", \"run2\"\n",
    "\n",
    "annFile = '{}{}.json'.format(DATADIR,dataSplit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "60104674-c925-4c87-a555-90fa0b03f594",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-14T12:34:39.802597Z",
     "iopub.status.busy": "2025-03-14T12:34:39.802299Z",
     "iopub.status.idle": "2025-03-14T12:34:58.162003Z",
     "shell.execute_reply": "2025-03-14T12:34:58.161032Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done (t=17.50s)\n",
      "creating index...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index created!\n"
     ]
    }
   ],
   "source": [
    "coco_val = COCO(annFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eea8492e-c022-4aec-a314-e723977faa60",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-14T12:34:58.165966Z",
     "iopub.status.busy": "2025-03-14T12:34:58.165482Z",
     "iopub.status.idle": "2025-03-14T12:34:58.169007Z",
     "shell.execute_reply": "2025-03-14T12:34:58.168300Z"
    }
   },
   "outputs": [],
   "source": [
    "dataSplit, TEST_RUN = \"run-full-test\", \"run3\"\n",
    "\n",
    "annFile = '{}{}.json'.format(DATADIR,dataSplit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e26cddc7-7894-4706-ab35-650913d04a6b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-14T12:34:58.172767Z",
     "iopub.status.busy": "2025-03-14T12:34:58.172294Z",
     "iopub.status.idle": "2025-03-14T12:35:08.329286Z",
     "shell.execute_reply": "2025-03-14T12:35:08.328405Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done (t=9.67s)\n",
      "creating index...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index created!\n"
     ]
    }
   ],
   "source": [
    "coco_test = COCO(annFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b2fdf2f3-61ca-4528-909c-5101c530783f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-14T12:35:08.333453Z",
     "iopub.status.busy": "2025-03-14T12:35:08.332921Z",
     "iopub.status.idle": "2025-03-14T12:35:12.589499Z",
     "shell.execute_reply": "2025-03-14T12:35:12.588382Z"
    }
   },
   "outputs": [],
   "source": [
    "player_positions = {\"run1\":[], \"run2\":[], \"run3\":[]}\n",
    "motion_vectors = {\"run1\":[], \"run2\":[], \"run3\":[]}\n",
    "\n",
    "for run in USED_RUNS:\n",
    "    with open(DATADIR+run+\"/log.txt\", 'r') as log_file:\n",
    "        for line in log_file:\n",
    "            if \"player\" in line:\n",
    "                line = line.strip()\n",
    "                tic, stats = line.split(\"player:\")\n",
    "                x, y, z, angle = stats.split(\",\")\n",
    "    \n",
    "                # Store position in the dictionary\n",
    "                player_positions[run].append((float(x), float(y), float(z), float(angle)))\n",
    "                if len(player_positions[run]) >= 2:\n",
    "                    player_position = player_positions[run][-1]\n",
    "                    prev_player_position = player_positions[run][-2]\n",
    "                    \n",
    "                    dx = player_position[0] - prev_player_position[0]\n",
    "                    dy = player_position[1] - prev_player_position[1]\n",
    "                    dz = player_position[2] - prev_player_position[2]\n",
    "                    dangle = np.pi - abs(abs(player_position[3] - prev_player_position[3]) - np.pi)\n",
    "                    \n",
    "                    dx_relative = dx * np.cos(2 * np.pi - prev_player_position[3]) + dy * np.cos(prev_player_position[3] - 1/2 * np.pi)\n",
    "                    dy_relative = dx * np.sin(2 * np.pi - prev_player_position[3]) + dy * np.sin(prev_player_position[3] - 1/2 * np.pi)\n",
    "                    motion_vector = (dx_relative, dy_relative, dz, dangle)\n",
    "                    motion_vectors[run].append(motion_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a78dd7f8-f3a5-4046-a31d-46f5c3cc747e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-14T12:35:12.593884Z",
     "iopub.status.busy": "2025-03-14T12:35:12.593600Z",
     "iopub.status.idle": "2025-03-14T12:35:12.608728Z",
     "shell.execute_reply": "2025-03-14T12:35:12.608039Z"
    }
   },
   "outputs": [],
   "source": [
    "class DoomMotionDataset(Dataset):\n",
    "    def __init__(self, coco, run, input_window, prediction_window, transform=None):\n",
    "        self.coco = coco\n",
    "        self.run = run\n",
    "        self.img_ids = self.coco.getImgIds()\n",
    "        self.transform = transform\n",
    "        self.input_window = input_window\n",
    "        self.prediction_window = prediction_window\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_ids)\n",
    "\n",
    "    def fullSegmentationFormat(self, rgb_filename):\n",
    "        seg_image = self.load_image(self.getSegmentationMask(DATADIR + rgb_filename))\n",
    "        seg_class_map = self.color_to_index(seg_image)\n",
    "        seg_class_one_hot = functions.one_hot(seg_class_map, num_classes=4).to(dtype=torch.float).permute(2, 0, 1)\n",
    "        return seg_class_one_hot\n",
    "\n",
    "    def fullDepthFormat(self, rgb_filename):\n",
    "        depth_mask = self.load_image(self.getDepthMask(DATADIR + rgb_filename))\n",
    "        depth_mask = torch.tensor(depth_mask, dtype=torch.float32)\n",
    "        return depth_mask\n",
    "\n",
    "    def getSegmentationMask(self, rgb_filename):\n",
    "        return rgb_filename.replace(\"rgb\", \"objects\")\n",
    "\n",
    "    def getDepthMask(self, rgb_filename):\n",
    "        return rgb_filename.replace(\"rgb\", \"depth\")\n",
    "\n",
    "    def color_to_index(self, segmentation_image):\n",
    "        # Map colors to class indices\n",
    "        r, g, b = segmentation_image\n",
    "        pixel_values = r + (g *  2**8) + (b * 2**16)  # From cocodoom documentation, converts to an object id\n",
    "\n",
    "        class_map = torch.full_like(pixel_values, 3, dtype=torch.long)\n",
    "\n",
    "        sky = (1 << 23) + 0\n",
    "        horizontal = (1 << 23) + 1\n",
    "        vertical = (1 << 23) + 2\n",
    "        \n",
    "        class_map[x == sky] = 0\n",
    "        class_map[x == horizontal] = 1\n",
    "        class_map[x == vertical] = 2\n",
    "        return class_map\n",
    "\n",
    "    def load_image(self, path):\n",
    "        if os.path.exists(path):\n",
    "            img = Image.open(path)\n",
    "            return transforms.ToTensor()(img)\n",
    "        return False\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load the RGB image\n",
    "        rgb_filename = self.coco.loadImgs(self.img_ids[idx])[0]['file_name']\n",
    "        #print(rgb_filename)\n",
    "        tic = int(rgb_filename.replace(\".png\", \"\").split(\"/\")[-1])\n",
    "        next_tic = tic+1\n",
    "        previous_tic = tic-1\n",
    "        prev_motion_vectors = []\n",
    "        next_motion_vectors = []\n",
    "        prev_seg = []\n",
    "        prev_dep = []\n",
    "\n",
    "        for t in range(input_window, 0, -1):\n",
    "            if tic-t < 0:\n",
    "                prev_motion_vectors.append(motion_vectors[self.run][0])\n",
    "                prev_filename = self.coco.loadImgs(self.img_ids[0])[0]['file_name']\n",
    "                seg = self.fullSegmentationFormat(prev_filename)\n",
    "                if seg != False: prev_seg.append(seg)\n",
    "                dep = self.fullDepthFormat(prev_filename)\n",
    "                if dep != False: prev_dep.append(dep)\n",
    "                continue\n",
    "            elif tic-t >= len(motion_vectors[self.run]):\n",
    "                prev_motion_vectors.append(motion_vectors[self.run][-1])\n",
    "                prev_filename = self.coco.loadImgs(self.img_ids[-1])[0]['file_name']\n",
    "                seg = self.fullSegmentationFormat(prev_filename)\n",
    "                if seg != False: prev_seg.append(seg)\n",
    "                dep = self.fullDepthFormat(prev_filename)\n",
    "                if dep != False: prev_dep.append(dep)\n",
    "                continue\n",
    "            prev_motion_vectors.append(motion_vectors[self.run][tic-t])\n",
    "            prev_filename = rgb_filename[:-10] + str(max(tic - t, 2)).rjust(6, \"0\") + \".png\"\n",
    "            seg = self.fullSegmentationFormat(prev_filename)\n",
    "            if seg != False: prev_seg.append(seg)\n",
    "            dep = self.fullDepthFormat(prev_filename)\n",
    "            if dep != False: prev_dep.append(dep)\n",
    "\n",
    "        for t in range(1, prediction_window+1):\n",
    "            if tic+t >= len(motion_vectors[self.run]):\n",
    "                next_motion_vectors.append(motion_vectors[self.run][-1])\n",
    "                continue\n",
    "            next_motion_vectors.append(motion_vectors[self.run][tic+t])\n",
    "\n",
    "        # if dx > 1000:\n",
    "        #     print(f\"idx: {idx}\")\n",
    "        #     print(f\"rgb_filename: {rgb_filename}\")\n",
    "        #     print(f\"tic: {tic}\")\n",
    "        #     print(f\"next_tic: {next_tic}\")\n",
    "        #     print(f\"previous_tic: {previous_tic}\")\n",
    "        #     print(f\"Sus {idx}\")\n",
    "        #     print(f\"prev_player_position: {prev_player_position}\")\n",
    "        #     print(f\"player_position: {player_position}\")\n",
    "        #     print(f\"next_player_position: {next_player_position}\")\n",
    "        #     print(f\"prev_motion_vector: {prev_motion_vector}\")\n",
    "        #     print(f\"next_motion_vector: {next_motion_vector}\")\n",
    "\n",
    "        #print(prev_motion_vectors)\n",
    "        #print(next_motion_vectors)\n",
    "            \n",
    "        prev_motion_vectors = torch.tensor(prev_motion_vectors, dtype=torch.float32)\n",
    "        next_motion_vectors = torch.tensor(next_motion_vectors, dtype=torch.float32)\n",
    "        prev_seg = torch.stack(prev_seg)\n",
    "        prev_dep = torch.stack(prev_dep)\n",
    "        \n",
    "        return {\"prev_motion\" : prev_motion_vectors, \"next_motion\" : next_motion_vectors, \"previous_seg\" : prev_seg, \"previous_dep\" : prev_dep}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8680de85-df84-4334-87be-8defcfc6e6fb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-14T12:35:12.612687Z",
     "iopub.status.busy": "2025-03-14T12:35:12.612455Z",
     "iopub.status.idle": "2025-03-14T12:35:12.624661Z",
     "shell.execute_reply": "2025-03-14T12:35:12.623695Z"
    }
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "  def __init__(self, batch_size, input_length, sequence_length, activation_function=functions.relu, device=torch.device(\"cpu\")):\n",
    "    super(NeuralNetwork, self).__init__()\n",
    "    self.batch_size = batch_size\n",
    "    self.input_length = input_length\n",
    "    self.sequence_length = sequence_length\n",
    "\n",
    "    # Encoder\n",
    "    # Conv layers\n",
    "    self.conv_seg = nn.Conv2d(4, 1, kernel_size=3, stride=2, padding=1, bias=False).to(device)\n",
    "    self.conv_dep = nn.Conv2d(1, 1, kernel_size=3, stride=2, padding=1, bias=False).to(device)\n",
    "\n",
    "    self.motion_fc = nn.Linear(4, 32).to(device)\n",
    "      \n",
    "    # Pre-fusion LSTMs\n",
    "    self.vis_LSTM = nn.LSTM(input_size=32000, hidden_size=256, batch_first=True).to(device)\n",
    "    self.inertia_LSTM = nn.LSTM(input_size=32, hidden_size=256, batch_first=True).to(device)\n",
    "\n",
    "    # Fusion LSTM\n",
    "    self.fusion_LSTM = nn.LSTM(input_size=512, hidden_size=256, batch_first=True).to(device)\n",
    "\n",
    "    # Decoder\n",
    "    self.de_motion_fc = nn.Linear(4, 32).to(device)\n",
    "    self.de_vis_LSTM = nn.LSTM(input_size=32, hidden_size=256, batch_first=True).to(device) #Unsure what the input size of this should be as it actually receives nothing\n",
    "    self.de_inertia_LSTM = nn.LSTM(input_size=32, hidden_size=256, batch_first=True).to(device)\n",
    "    self.de_fusion_LSTM = nn.LSTM(input_size=512, hidden_size=256, batch_first=True).to(device)\n",
    "    self.output_fc = nn.Linear(256, 4).to(device)\n",
    "\n",
    "  def forward(self, segmentation, depth, prev_motion):\n",
    "    hidden_vis = None\n",
    "    hidden_inert = None\n",
    "    hidden_fus = None\n",
    "    \n",
    "    for t in range(self.input_length):\n",
    "        #print(segmentation.shape)\n",
    "        seg = self.conv_seg(segmentation[:,t])\n",
    "        #print(seg.shape)\n",
    "        dep = self.conv_dep(depth[:,t])\n",
    "        #print(dep.shape)\n",
    "        mot = self.motion_fc(prev_motion[:,t])\n",
    "        vis = torch.cat((seg, dep), dim=1)\n",
    "        vis = torch.flatten(vis, start_dim=1)\n",
    "        #print(vis.shape)\n",
    "        if hidden_vis != None:\n",
    "            output_vis, hidden_vis = self.vis_LSTM(vis, hidden_vis)\n",
    "        else:\n",
    "            output_vis, hidden_vis = self.vis_LSTM(vis)\n",
    "        if hidden_inert != None:\n",
    "            output_inert, hidden_inert = self.inertia_LSTM(mot, hidden_inert)\n",
    "        else:\n",
    "            output_inert, hidden_inert = self.inertia_LSTM(mot)\n",
    "        combined = torch.cat((output_vis, output_inert), dim=1)\n",
    "        if hidden_fus != None:\n",
    "            _, hidden_fus = self.fusion_LSTM(combined, hidden_fus)\n",
    "        else:\n",
    "            _, hidden_fus = self.fusion_LSTM(combined)\n",
    "\n",
    "    #print(\"Prev motion: \" + str(prev_motion.shape))\n",
    "    de_mot = prev_motion[:,-1]\n",
    "    output_tensor = torch.zeros(self.sequence_length, self.batch_size, 4).to(segmentation.device)\n",
    "    for t in range(self.sequence_length):\n",
    "        #print(de_mot.shape)\n",
    "        de_mot = self.de_motion_fc(de_mot)\n",
    "        de_output_inert, hidden_inert = self.de_inertia_LSTM(de_mot, hidden_inert)\n",
    "        de_output_vis, hidden_vis = self.de_vis_LSTM(torch.zeros(self.batch_size, 32).to(segmentation.device), hidden_vis)\n",
    "        #print(de_output_vis.shape, de_output_inert.shape)\n",
    "        combined = torch.cat((de_output_vis, de_output_inert), dim=1)\n",
    "        de_output_fus, hidden_fus = self.de_fusion_LSTM(combined, hidden_fus)\n",
    "        #print(\"de_output_fus: \" + str(de_output_fus.shape))\n",
    "        output_t = self.output_fc(de_output_fus)\n",
    "        #print(\"output_t: \" + str(output_t.shape))\n",
    "        #output_t = output_t.unsqueeze(0)\n",
    "        de_mot = output_t\n",
    "        output_tensor[t] = output_t.unsqueeze(0)\n",
    "        \n",
    "    return output_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "81129192-3f18-4f79-b559-be1db7b544d9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-14T12:35:12.628939Z",
     "iopub.status.busy": "2025-03-14T12:35:12.628667Z",
     "iopub.status.idle": "2025-03-14T12:35:14.447679Z",
     "shell.execute_reply": "2025-03-14T12:35:14.446886Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/10:   0%|                                                                            | 0/991 [00:00<?, ?batch/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch 1/10:   0%|                                                                            | 0/991 [00:00<?, ?batch/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Boolean value of Tensor with more than one value is ambiguous",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 25\u001b[0m\n\u001b[1;32m     23\u001b[0m running_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m     24\u001b[0m progress_bar \u001b[38;5;241m=\u001b[39m tqdm(train_loader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 25\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(progress_bar):\n\u001b[1;32m     26\u001b[0m     prev_motion, next_motion, previous_seg, previous_dep \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprev_motion\u001b[39m\u001b[38;5;124m\"\u001b[39m], batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnext_motion\u001b[39m\u001b[38;5;124m\"\u001b[39m], batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprevious_seg\u001b[39m\u001b[38;5;124m\"\u001b[39m], batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprevious_dep\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     27\u001b[0m     prev_motion, next_motion, previous_seg, previous_dep \u001b[38;5;241m=\u001b[39m prev_motion\u001b[38;5;241m.\u001b[39mto(device), next_motion\u001b[38;5;241m.\u001b[39mto(device), previous_seg\u001b[38;5;241m.\u001b[39mto(device), previous_dep\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m/shared/storage/cs/studentscratch/js3921/python_environments/envs/project_env_cuda/lib/python3.9/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m/shared/storage/cs/studentscratch/js3921/python_environments/envs/project_env_cuda/lib/python3.9/site-packages/torch/utils/data/dataloader.py:708\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    707\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 708\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    709\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    710\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    711\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    712\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    713\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    714\u001b[0m ):\n",
      "File \u001b[0;32m/shared/storage/cs/studentscratch/js3921/python_environments/envs/project_env_cuda/lib/python3.9/site-packages/torch/utils/data/dataloader.py:764\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    762\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    763\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 764\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    765\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    766\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/shared/storage/cs/studentscratch/js3921/python_environments/envs/project_env_cuda/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/shared/storage/cs/studentscratch/js3921/python_environments/envs/project_env_cuda/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[10], line 69\u001b[0m, in \u001b[0;36mDoomMotionDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     67\u001b[0m prev_filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoco\u001b[38;5;241m.\u001b[39mloadImgs(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimg_ids[\u001b[38;5;241m0\u001b[39m])[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfile_name\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     68\u001b[0m seg \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfullSegmentationFormat(prev_filename)\n\u001b[0;32m---> 69\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m seg: prev_seg\u001b[38;5;241m.\u001b[39mappend(seg)\n\u001b[1;32m     70\u001b[0m dep \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfullDepthFormat(prev_filename)\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dep: prev_dep\u001b[38;5;241m.\u001b[39mappend(dep)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Boolean value of Tensor with more than one value is ambiguous"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "batch_size = 256\n",
    "learning_rate = 1e-3\n",
    "num_epochs = 10\n",
    "input_window = 5\n",
    "prediction_window = 10\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = NeuralNetwork(batch_size, input_window, prediction_window, device=device).to(device)\n",
    "\n",
    "train_dataset = DoomMotionDataset(coco_train, TRAIN_RUN, input_window, prediction_window)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "val_dataset = DoomMotionDataset(coco_val, VAL_RUN, input_window, prediction_window)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\", unit=\"batch\")\n",
    "    for batch_idx, batch in enumerate(progress_bar):\n",
    "        prev_motion, next_motion, previous_seg, previous_dep = batch[\"prev_motion\"], batch[\"next_motion\"], batch[\"previous_seg\"], batch[\"previous_dep\"]\n",
    "        prev_motion, next_motion, previous_seg, previous_dep = prev_motion.to(device), next_motion.to(device), previous_seg.to(device), previous_dep.to(device)\n",
    "\n",
    "        if prev_motion.shape[0] != next_motion.shape[0]:\n",
    "                continue\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(previous_seg, previous_dep, prev_motion)\n",
    "        outputs = outputs.permute(1, 0, 2)\n",
    "\n",
    "        if outputs.size(0) != next_motion.size(0):\n",
    "            continue\n",
    "        \n",
    "        loss = criterion(outputs, next_motion)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        progress_bar.set_postfix({\n",
    "            \"batch_loss\": loss.item(),\n",
    "            \"batch_index\": batch_idx + 1,\n",
    "            \"batch_size\": prev_motion.size(0)\n",
    "        })\n",
    "\n",
    "    # Average loss per epoch\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1}, Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    \n",
    "    progress_bar = tqdm(val_loader, desc=\"Validation\", unit=\"batch\")\n",
    "    \n",
    "    with torch.no_grad():  # Disable gradient calculations for evaluation\n",
    "        for batch_idx, batch in enumerate(progress_bar):\n",
    "            prev_motion, next_motion, previous_seg, previous_dep = batch[\"prev_motion\"], batch[\"next_motion\"], batch[\"previous_seg\"], batch[\"previous_dep\"]\n",
    "            prev_motion, next_motion, previous_seg, previous_dep = prev_motion.to(device), next_motion.to(device), previous_seg.to(device), previous_dep.to(device)\n",
    "\n",
    "            if prev_motion.shape[0] != next_motion.shape[0]:\n",
    "                continue\n",
    "                \n",
    "            outputs = model(previous_seg, previous_dep, prev_motion)\n",
    "            outputs = outputs.permute(1, 0, 2)\n",
    "\n",
    "            if outputs.size(0) != next_motion.size(0):\n",
    "                continue\n",
    "            \n",
    "            loss = criterion(outputs, next_motion)\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            progress_bar.set_postfix({\n",
    "                \"batch_loss\": loss.item(),\n",
    "                \"batch_index\": batch_idx + 1,\n",
    "                \"batch_size\": prev_motion.size(0)\n",
    "            })\n",
    "    \n",
    "    # Average loss over all batches\n",
    "    val_loss = running_loss / len(val_loader)\n",
    "    print(f\"Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(model.state_dict(), \"multimodal_seq2seq.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1ce70e48-3ccb-4628-91f7-19fd56e7f6aa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-14T12:35:14.451978Z",
     "iopub.status.busy": "2025-03-14T12:35:14.451775Z",
     "iopub.status.idle": "2025-03-14T12:35:15.046134Z",
     "shell.execute_reply": "2025-03-14T12:35:15.045211Z"
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'multimodal_seq2seq.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m criterion \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mMSELoss()\n\u001b[1;32m      3\u001b[0m model \u001b[38;5;241m=\u001b[39m NeuralNetwork(batch_size, input_window, prediction_window, device\u001b[38;5;241m=\u001b[39mdevice)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m----> 4\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmultimodal_seq2seq.pth\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m)\n\u001b[1;32m      6\u001b[0m test_dataset \u001b[38;5;241m=\u001b[39m DoomMotionDataset(coco_test, TEST_RUN, input_window, prediction_window)\n\u001b[1;32m      7\u001b[0m test_loader \u001b[38;5;241m=\u001b[39m DataLoader(test_dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m/shared/storage/cs/studentscratch/js3921/python_environments/envs/project_env_cuda/lib/python3.9/site-packages/torch/serialization.py:1425\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1422\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m   1423\u001b[0m     pickle_load_args[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1425\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[1;32m   1426\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m   1427\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m   1428\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m   1429\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m   1430\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[0;32m/shared/storage/cs/studentscratch/js3921/python_environments/envs/project_env_cuda/lib/python3.9/site-packages/torch/serialization.py:751\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    749\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    750\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 751\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    752\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    753\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[0;32m/shared/storage/cs/studentscratch/js3921/python_environments/envs/project_env_cuda/lib/python3.9/site-packages/torch/serialization.py:732\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    731\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[0;32m--> 732\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'multimodal_seq2seq.pth'"
     ]
    }
   ],
   "source": [
    "device = (torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "criterion = torch.nn.MSELoss()\n",
    "model = NeuralNetwork(batch_size, input_window, prediction_window, device=device).to(device)\n",
    "model.load_state_dict(torch.load(\"multimodal_seq2seq.pth\", weights_only=True))\n",
    "\n",
    "test_dataset = DoomMotionDataset(coco_test, TEST_RUN, input_window, prediction_window)\n",
    "test_loader = DataLoader(test_dataset, batch_size=256, shuffle=False)\n",
    "\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "running_loss = 0.0\n",
    "\n",
    "\n",
    "progress_bar = tqdm(test_loader, desc=\"Testing\", unit=\"batch\")\n",
    "\n",
    "with torch.no_grad():  # Disable gradient calculations for evaluation\n",
    "    for batch_idx, batch in enumerate(progress_bar):\n",
    "        prev_motion, next_motion, previous_seg, previous_dep = batch[\"prev_motion\"], batch[\"next_motion\"], batch[\"previous_seg\"], batch[\"previous_dep\"]\n",
    "        prev_motion, next_motion, previous_seg, previous_dep = prev_motion.to(device), next_motion.to(device), previous_seg.to(device), previous_dep.to(device)\n",
    "\n",
    "        if prev_motion.shape[0] != next_motion.shape[0]:\n",
    "                continue\n",
    "            \n",
    "        outputs = model(previous_seg, previous_dep, prev_motion)\n",
    "        outputs = outputs.permute(1, 0, 2)\n",
    "\n",
    "        if outputs.size(0) != next_motion.size(0):\n",
    "            continue\n",
    "        \n",
    "        loss = criterion(outputs, next_motion)\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        progress_bar.set_postfix({\n",
    "            \"batch_loss\": loss.item(),\n",
    "            \"batch_index\": batch_idx + 1,\n",
    "            \"batch_size\": prev_motion.size(0)\n",
    "        })\n",
    "\n",
    "# Average loss over all batches\n",
    "test_loss = running_loss / len(test_loader)\n",
    "print(f\"Test Loss: {test_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f38629-2cd1-4873-9084-8683c5716ad5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
