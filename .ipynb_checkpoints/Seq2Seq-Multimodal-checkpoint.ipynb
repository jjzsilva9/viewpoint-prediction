{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "caac3c60-cbd9-4794-a8c0-ced5b193b331",
   "metadata": {},
   "source": [
    "<h1>NN Model</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c437917-e990-4491-a25e-b7adeb77f372",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycocotools.coco import COCO\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as functions\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "#import torchvision\n",
    "#from torchvision import transforms\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "edca0610-7a84-420b-81d6-9d3c6063539a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "print(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9003254-3d2c-4f84-91a9-0c6d966f104e",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATADIR = \"cocodoom/\"\n",
    "USED_RUNS = [\"run1\", \"run2\", \"run3\"]\n",
    "\n",
    "dataSplit, TRAIN_RUN = \"run-full-train\", \"run1\"\n",
    "\n",
    "annFile = '{}{}.json'.format(DATADIR,dataSplit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e8df8e50-550c-43a3-8b8e-b5a5e41ed0b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=22.90s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "coco_train = COCO(annFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eeebf160-b14d-425d-bdc9-48f1f985af94",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataSplit, VAL_RUN = \"run-full-val\", \"run2\"\n",
    "\n",
    "annFile = '{}{}.json'.format(DATADIR,dataSplit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "60104674-c925-4c87-a555-90fa0b03f594",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=20.68s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "coco_val = COCO(annFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eea8492e-c022-4aec-a314-e723977faa60",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataSplit, TEST_RUN = \"run-full-test\", \"run3\"\n",
    "\n",
    "annFile = '{}{}.json'.format(DATADIR,dataSplit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e26cddc7-7894-4706-ab35-650913d04a6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=15.18s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "coco_test = COCO(annFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b2fdf2f3-61ca-4528-909c-5101c530783f",
   "metadata": {},
   "outputs": [],
   "source": [
    "player_positions = {\"run1\":[], \"run2\":[], \"run3\":[]}\n",
    "motion_vectors = {\"run1\":[], \"run2\":[], \"run3\":[]}\n",
    "\n",
    "for run in USED_RUNS:\n",
    "    with open(DATADIR+run+\"/log.txt\", 'r') as log_file:\n",
    "        for line in log_file:\n",
    "            if \"player\" in line:\n",
    "                line = line.strip()\n",
    "                tic, stats = line.split(\"player:\")\n",
    "                x, y, z, angle = stats.split(\",\")\n",
    "    \n",
    "                # Store position in the dictionary\n",
    "                player_positions[run].append((float(x), float(y), float(z), float(angle)))\n",
    "                if len(player_positions[run]) >= 2:\n",
    "                    player_position = player_positions[run][-1]\n",
    "                    prev_player_position = player_positions[run][-2]\n",
    "                    \n",
    "                    dx = player_position[0] - prev_player_position[0]\n",
    "                    dy = player_position[1] - prev_player_position[1]\n",
    "                    dz = player_position[2] - prev_player_position[2]\n",
    "                    dangle = np.pi - abs(abs(player_position[3] - prev_player_position[3]) - np.pi)\n",
    "                    \n",
    "                    dx_relative = dx * np.cos(2 * np.pi - prev_player_position[3]) + dy * np.cos(prev_player_position[3] - 1/2 * np.pi)\n",
    "                    dy_relative = dx * np.sin(2 * np.pi - prev_player_position[3]) + dy * np.sin(prev_player_position[3] - 1/2 * np.pi)\n",
    "                    motion_vector = (dx_relative, dy_relative, dz, dangle)\n",
    "                    motion_vectors[run].append(motion_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a78dd7f8-f3a5-4046-a31d-46f5c3cc747e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoomMotionDataset(Dataset):\n",
    "    def __init__(self, coco, run, input_window, prediction_window, transform=None):\n",
    "        self.coco = coco\n",
    "        self.run = run\n",
    "        self.img_ids = self.coco.getImgIds()\n",
    "        self.transform = transform\n",
    "        self.input_window = input_window\n",
    "        self.prediction_window = prediction_window\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load the RGB image\n",
    "        rgb_filename = self.coco.loadImgs(self.img_ids[idx])[0]['file_name']\n",
    "        tic = int(rgb_filename.replace(\".png\", \"\").split(\"/\")[-1])\n",
    "        next_tic = tic+1\n",
    "        previous_tic = tic-1\n",
    "        prev_motion_vectors = []\n",
    "        next_motion_vectors = []\n",
    "\n",
    "        for t in range(input_window, 0, -1):\n",
    "            if tic-t < 0:\n",
    "                prev_motion_vectors.append(motion_vectors[self.run][0])\n",
    "                continue\n",
    "            elif tic-t >= len(motion_vectors[self.run]):\n",
    "                prev_motion_vectors.append(motion_vectors[self.run][-1])\n",
    "                continue\n",
    "            prev_motion_vectors.append(motion_vectors[self.run][tic-t])\n",
    "\n",
    "        for t in range(1, prediction_window+1):\n",
    "            if tic+t >= len(motion_vectors[self.run]):\n",
    "                next_motion_vectors.append(motion_vectors[self.run][-1])\n",
    "                continue\n",
    "            next_motion_vectors.append(motion_vectors[self.run][tic+t])\n",
    "\n",
    "        # if dx > 1000:\n",
    "        #     print(f\"idx: {idx}\")\n",
    "        #     print(f\"rgb_filename: {rgb_filename}\")\n",
    "        #     print(f\"tic: {tic}\")\n",
    "        #     print(f\"next_tic: {next_tic}\")\n",
    "        #     print(f\"previous_tic: {previous_tic}\")\n",
    "        #     print(f\"Sus {idx}\")\n",
    "        #     print(f\"prev_player_position: {prev_player_position}\")\n",
    "        #     print(f\"player_position: {player_position}\")\n",
    "        #     print(f\"next_player_position: {next_player_position}\")\n",
    "        #     print(f\"prev_motion_vector: {prev_motion_vector}\")\n",
    "        #     print(f\"next_motion_vector: {next_motion_vector}\")\n",
    "\n",
    "        #print(prev_motion_vectors)\n",
    "        #print(next_motion_vectors)\n",
    "            \n",
    "        prev_motion_vectors = torch.tensor(prev_motion_vectors, dtype=torch.float32)\n",
    "        next_motion_vectors = torch.tensor(next_motion_vectors, dtype=torch.float32)\n",
    "        \n",
    "        return prev_motion_vectors, next_motion_vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8680de85-df84-4334-87be-8defcfc6e6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "  def __init__(self, batch_size, input_length, sequence_length, activation_function=functions.relu, device=torch.device(\"cpu\")):\n",
    "    super(NeuralNetwork, self).__init__()\n",
    "    self.batch_size = batch_size\n",
    "    self.input_length = input_length\n",
    "    self.sequence_length = sequence_length\n",
    "\n",
    "    # Encoder\n",
    "    # Conv layers\n",
    "    self.conv_seg = nn.Conv2d(4, 32, kernel_size=3, stride=2, padding=1, bias=False).to(device)\n",
    "    self.conv_dep = nn.Conv2d(1, 32, kernel_size=3, stride=2, padding=1, bias=False).to(device)\n",
    "\n",
    "    self.motion_fc = nn.Linear(4, 32).to(device)\n",
    "      \n",
    "    # Pre-fusion LSTMs\n",
    "    self.vis_LSTM(input_size=32, hidden_size=256, batch_first=True).to(device)\n",
    "    self.inertia_LSTM(input_size=32, hidden_size=256, batch_first=True).to(device)\n",
    "\n",
    "    # Fusion LSTM\n",
    "    self.fusion_LSTM(input_size=64, hidden_size=256, batch_first=True).to(device)\n",
    "\n",
    "    # Decoder\n",
    "    self.de_motion_fc = nn.Linear(4, 32).to(device)\n",
    "    self.de_vis_LSTM(input_size=0, hidden_size=256, batch_first=True).to(device)\n",
    "    self.de_inertia_LSTM(input_size=32, hidden_size=256, batch_first=True).to(device)\n",
    "    self.de_fusion_LSTM(input_size=64, hidden_size=256, batch_first=True).to(device)\n",
    "    self.output_fc = nn.Linear(256, 4).to(device)\n",
    "\n",
    "  def forward(self, segmentation, depth, prev_motion):\n",
    "    hidden_vis = None\n",
    "    hidden_inert = None\n",
    "    hidden_fus = None\n",
    "    seg = self.conv_seg(segmentation)\n",
    "    dep = self.conv_dep(depth)\n",
    "    mot = self.motion_fc(prev_motion[:,:-1])\n",
    "    for t in range(self.input_length):\n",
    "        vis = torch.cat((seg[:,t], dep[:,t]), dim=1)\n",
    "        if hidden_vis != None:\n",
    "            output_vis, hidden_vis = self.vis_LSTM(vis, hidden_vis)\n",
    "        else:\n",
    "            output_vis, hidden_vis = self.vis_LSTM(vis)\n",
    "        if hidden_inert != None:\n",
    "            output_inert, hidden_inert = self.inertia_LSTM(mot[:,t], hidden_inert)\n",
    "        else:\n",
    "            output_inert, hidden_inert = self.inertia_LSTM(mot[:,t])\n",
    "        combined = torch.cat((output_vis, output_inert), dim=1)\n",
    "        if hidden_fus != None:\n",
    "            _, hidden_fus = self.fusion_LSTM(combined, hidden_fus)\n",
    "        else:\n",
    "            _, hidden_fus = self.fusion_LSTM(combined)\n",
    "\n",
    "    de_mot = prev_motion[:,-1]\n",
    "    output_tensor = torch.zeros(self.sequence_length, self.batch_size, 4).to(x.device)\n",
    "    for t in range(self.sequence_length):\n",
    "        de_mot = self.de_motion_fc(de_mot)\n",
    "        de_output_inert, hidden_inert = self.de_inertia_LSTM(de_mot, hidden_inert)\n",
    "        de_output_vis, hidden_vis = self.de_vis_LSTM(torch.tensor(), hidden_vis)\n",
    "        combined = torch.cat((de_output_vis, de_output_inert), dim=1)\n",
    "        de_output_fus, hidden_fus = self.de_fusion_LSTM(combined, hidden_fus)\n",
    "        output_t = self.fc(de_output_fus[-1])\n",
    "        output_t = output_t.unsqueeze(0)\n",
    "        output_tensor[t] = output_t.unsqueeze(0)\n",
    "        \n",
    "    return output_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81129192-3f18-4f79-b559-be1db7b544d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   1%|██                                                                                                                                                                             | 12/991 [00:15<21:21,  1.31s/batch, batch_loss=2.01e+3, batch_index=12, batch_size=256]"
     ]
    }
   ],
   "source": [
    "batch_size = 256\n",
    "learning_rate = 1e-3\n",
    "num_epochs = 10\n",
    "input_window = 5\n",
    "prediction_window = 10\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = NeuralNetwork(batch_size, input_window, prediction_window, device=device).to(device)\n",
    "\n",
    "train_dataset = DoomMotionDataset(coco_train, TRAIN_RUN, input_window, prediction_window)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "val_dataset = DoomMotionDataset(coco_val, VAL_RUN, input_window, prediction_window)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\", unit=\"batch\")\n",
    "    for batch_idx, (inputs, targets) in enumerate(progress_bar):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        #print(inputs.shape)\n",
    "        #print(targets.shape)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        outputs = outputs.permute(1, 0, 2)\n",
    "        \n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        progress_bar.set_postfix({\n",
    "            \"batch_loss\": loss.item(),\n",
    "            \"batch_index\": batch_idx + 1,\n",
    "            \"batch_size\": inputs.size(0)\n",
    "        })\n",
    "\n",
    "    # Average loss per epoch\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1}, Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    \n",
    "    progress_bar = tqdm(val_loader, desc=\"Validation\", unit=\"batch\")\n",
    "    \n",
    "    with torch.no_grad():  # Disable gradient calculations for evaluation\n",
    "        for batch_idx, (inputs, targets) in enumerate(progress_bar):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            outputs = outputs.permute(1, 0, 2)\n",
    "            \n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            progress_bar.set_postfix({\n",
    "                \"batch_loss\": loss.item(),\n",
    "                \"batch_index\": batch_idx + 1,\n",
    "                \"batch_size\": inputs.size(0)\n",
    "            })\n",
    "    \n",
    "    # Average loss over all batches\n",
    "    val_loss = running_loss / len(val_loader)\n",
    "    print(f\"Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(model.state_dict(), \"movement_seq2seq.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce70e48-3ccb-4628-91f7-19fd56e7f6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = (torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "criterion = torch.nn.MSELoss()\n",
    "model = NeuralNetwork(batch_size, input_window, prediction_window, device=device).to(device)\n",
    "model.load_state_dict(torch.load(\"movement_seq2seq.pth\", weights_only=True))\n",
    "\n",
    "test_dataset = DoomMotionDataset(coco_test, TEST_RUN, input_window, prediction_window)\n",
    "test_loader = DataLoader(test_dataset, batch_size=256, shuffle=False)\n",
    "\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "running_loss = 0.0\n",
    "\n",
    "\n",
    "progress_bar = tqdm(test_loader, desc=\"Testing\", unit=\"batch\")\n",
    "\n",
    "with torch.no_grad():  # Disable gradient calculations for evaluation\n",
    "    for batch_idx, (inputs, targets) in enumerate(progress_bar):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "        outputs = outputs.permute(1, 0, 2)\n",
    "        \n",
    "        loss = criterion(outputs, targets)\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        progress_bar.set_postfix({\n",
    "            \"batch_loss\": loss.item(),\n",
    "            \"batch_index\": batch_idx + 1,\n",
    "            \"batch_size\": inputs.size(0)\n",
    "        })\n",
    "\n",
    "# Average loss over all batches\n",
    "test_loss = running_loss / len(test_loader)\n",
    "print(f\"Test Loss: {test_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f38629-2cd1-4873-9084-8683c5716ad5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
